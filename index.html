<!DOCTYPE html>
<!-- saved from url=(0049)http://people.eng.unimelb.edu.au/tcohn/index.html -->
<html class="gr__people_eng_unimelb_edu_au"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>Dr Vu Nguyen</title>

    <link href="./css/bootstrap.min.css" rel="stylesheet" media="screen">
    <link href="./css/bootstrap-glyphicons.css" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- MathJax -->
    <script async="" src="./js/analytics.js"></script><script type="text/javascript" src="./js/MathJax.js">
    </script>

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  <style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style></head>
  <body data-gr-c-s-loaded="true"><div id="MathJax_Message" style="display: none;"></div>
    <!-- JavaScript plugins (requires jQuery) -->
    <script src="./js/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./js/bootstrap.min.js"></script>

    <div class="container">
      <div class="row">
        <div class="col-sm-2">
          <br><br> 
<img src="./img/person/Vu_2016.jpg" class="img-responsive" alt="climb-sm">
<ul class="nav navbar-inverse">
  <li>
    <a href="index.html">Home</a>
  </li>
  <li>
    <a href="publications.html">Publications</a>
  </li>
  <li>
    <a href="CV.html">CV</a>
  </li>
</ul>


        </div>
        <div class="col-sm-10">
          <div class="page-header">
  <div class="row">
    <div class="col-sm-3">
      <h3>Dr Vu Nguyen</h3>
      <address>
        <br>
        <br>
      </address>
    </div>
    <div class="col-sm-9">
      <br>
      <div class="row">
        <div class="col-sm-4">
          Associate Research Fellow <br>
        </div>
        <div class="col-sm-6">
          <a href="http://www.deakin.edu.au/research/prada">Center for Pattern Recognition and Data Analytics</a><br>
          <a href="http://deakin.edu.au">Deakin University</a><br>
        </div>
      </div>
    </div>
  </div>
  <div class="row">
    <div class="col-sm-2">
	<script type="text/javascript"><!--
	document.write('<a href="' +
'&#109;&#097;&#105;&#108;&#116;&#111;&#058;&#116;' +
'&#099;&#111;&#104;&#110;&#064;&#117;&#110;&#105;' +
'&#109;&#101;&#108;&#098;&#046;&#101;&#100;&#117;' +
'&#046;&#097;&#117;&#013;&#010;">'
);
//-->

	</script>	<a href="mailto:v.nguyen@deakin.edu.au"><span class="glyphicon glyphicon-envelope"></span></a> Email
	<noscript>&lt;IMG alt="E-mail" border=0 src="./email.jpg"&gt;</noscript>
    </div>
	
    <div class="col-sm-2">
        <a href="https://scholar.google.com.au/citations?user=5RQyC9cAAAAJ&hl=en">
          <img src="./img/ico/gs.png" alt="">
        </a>
        Google Scholar
    </div>
	
	<div class="col-sm-2">
        <a href="https://github.com/ntienvu">
          <img src="./img/ico/github_icon.png" alt="">
        </a>
        Github
    </div>
	
	<div class="col-sm-2">
        <a href="mailto:v.nguyen@deakin.edu.au">v.nguyen@deakin.edu.au</a>
    </div>
	
	
  </div>
</div>

<p>
 I am currently working as an Associate Research Fellow at <a href="http://www.deakin.edu.au/research/prada">PRADA</a>, <a href="http://www.deakin.edu.au/">Deakin University</a> with <a href="http://prada-research.net/~svetha/">ARC Laureate Prof Svetha Venkatesh</a>, Dr Sunil Gupta and Dr Santu Rana.
 I finished my PhD in the same department where I am very fortunate to be advised by Prof. <a href="http://prada-research.net/~dinh/">Dinh Phung</a> and Prof. <a href="http://prada-research.net/~svetha/">Svetha Venkatesh</a> in 2015.
</p>

<h4>Recruitment</h4>
<ul>
      <li>
        Several <strong>Postdoc Positions</strong> available in Machine Learning, Artificial Intelligent and Bayesian Optimization. AUD $83k+ pa for 2-5 years.
      </li>

      <li>
		Several <strong>PhD Scholarships</strong> available in Machine Learning, Artificial Intelligent and Bayesian Optimization. AUD $26k+ pa for 3 years.
      </li>
	  
	  <li>
	  These positions are at <a href="http://www.deakin.edu.au/research/prada">PRADA</a> under Prof. <a href="http://prada-research.net/~svetha/">Svetha Venkatesh</a>.   
	  Please email your CV to <a href="mailto:v.nguyen@deakin.edu.au">v.nguyen@deakin.edu.au</a>  if you are interested in.
	   </li>

</ul>


<h4>Research Interests</h4>
<ul>
	    <li>
			  Bayesian Optimization
      </li>
	  
      <li>
	  Bayesian Nonparametric, Multilevel Modelling, Label-drift Classification, 		Abnormality Detection

      </li>

	  
</ul>

<h4>Awards</h4>
<ul>
      <li>
        <strong>Vice Chancellor Award for Outstanding Contribution</strong>, Deakin University 2017
      </li>
	  
		<li>
        <strong>Selected as Best Papers for KAIS - <i>ICDM 2017</i></strong> 
      </li>
	  
      <li>
        <strong>Best Poster Award - <i>ICME 2017</i></strong>, 4th World Congress on Integrated Computational Materials Engineering 2017
      </li>

      <li>
		<strong>Best Paper Runner up Award and Best Poster Award - <i>ACML 2016</i></strong>
      </li>
	  
		<li>
		<strong>Finalist Best Intel Student Paper Award - <i>Track 5 ICPR 2016</i></strong>
      </li>
	  
		<li>
		<strong>Finalist Best IBM Student Paper Award – <i>Track 1 ICPR 2016</i></strong>
      </li>
	  
	  <li>
		<strong>Heidelberg Laurate Forum 2015</strong>, Top 200 young scientist around the world to interact with the Laurates in Germany
      </li>
	  
	  <li>
		<strong>First Prize in Student Research Competition 2011</strong>, University of Science, Vietnam National University HCM
      </li>
	  
	  <li>
		<strong>Travel Grant <i>Machine Learning Summer School, Singapore, 2011</i>.</strong>
      </li>
</ul>

<h4>Selected Papers</h4>



<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v77/nguyen17a/nguyen17a.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Regret for Expected Improvement over the Best-Observed Value and Stopping Condition</strong><br>

        In           <i>Proceedings of The 9th Asian Conference on Machine Learning</i>, (<strong>ACML</strong>),  pp. 279-294,    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractACML17" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractACML17" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Regret for Expected Improvement over the Best-Observed Value and Stopping Condition</h4>
            </div>
            <div class="modal-body">
Bayesian optimization (BO) is a sample-efficient method for global optimization of expensive,
noisy, black-box functions using probabilistic methods. The performance of a BO method depends
on its selection strategy through the acquisition function. Expected improvement (EI) is one of
the most widely used acquisition functions for BO that finds the expectation of the improvement
function over the incumbent. The incumbent is usually selected as the best-observed value so far,
termed as y^max (for the maximizing problem). Recent work has studied the convergence rate for
EI under some mild assumptions or zero noise of observations. Especially, the work of Wang and
de Freitas (2014) has derived the sublinear regret for EI under a stochastic noise. However, due to
the difficulty in stochastic noise setting and to make the convergent proof feasible, they use an alternative
choice for the incumbent as the maximum of the Gaussian process predictive mean, \mu^max.
This modification makes the algorithm computationally inefficient because it requires an additional
global optimization step to estimate mmax that is costly and may be inaccurate. To address this
issue, we derive a sublinear convergence rate for EI using the commonly used ymax. Moreover, our
analysis is the first to study a stopping criteria for EI to prevent unnecessary evaluations. Our analysis
complements the results of Wang and de Freitas (2014) to theoretically cover two incumbent
settings for EI. Finally, we demonstrate empirically that EI using y^max is both more computationally
efficiency and more accurate than EI using \mu^max.
			</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="http://proceedings.mlr.press/v77/nguyen17a/nguyen17a.pdf" class="label label-info">PDF</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/8215507/">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Bayesian Optimization in Weakly Specified Search Space</strong><br>

        In           <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  accepted,    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICDM17_FBO" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICDM17_FBO" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Optimization in Weakly Specified Search Space</h4>
            </div>
            <div class="modal-body">
Bayesian optimization (BO) has recently emerged
as a powerful and flexible tool for hyper-parameter tuning and
more generally for the efficient global optimization of expensive
black-box functions. Systems implementing BO has successfully
solved difficult problems in automatic design choices and machine
learning hyper-parameters tunings. Many recent advances in
the methodologies and theories underlying Bayesian optimization
have extended the framework to new applications and provided
greater insights into the behavior of these algorithms. Still, these
established techniques always require a user-defined space to
perform optimization. This pre-defined space specifies the ranges
of hyper-parameter values. In many situations, however, it can
be difficult to prescribe such spaces, as a prior knowledge is
often unavailable. Setting these regions arbitrarily can lead to
inefficient optimization - if a space is too large, we can miss the
optimum with a limited budget, on the other hand, if a space is
too small, it may not contain the optimum point that we want to
get. The unknown search space problem is intractable to solve in
practice. Therefore, in this paper, we narrow down to consider
specifically the setting of “weakly specified” search space for
Bayesian optimization. By weakly specified space, we mean that
the pre-defined space is placed at a sufficiently good region so that
the optimization can expand and reach to the optimum. However,
this pre-defined space need not include the global optimum.
We tackle this problem by proposing the filtering expansion
strategy for Bayesian optimization. Our approach starts from
the initial region and gradually expands the search space. We
develop an efficient algorithm for this strategy and derive its
regret bound. These theoretical results are complemented by an
extensive set of experiments on benchmark functions and two
real-world applications which demonstrate the benefits of our
proposed approach.</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://github.com/ntienvu/ICDM2017_FBO" class="label label-success">Code</a>
		<a href="" class="label label-danger">Selected as Best Papers, Invited for KAIS</a>
			  <a href="http://ieeexplore.ieee.org/document/8215507/" class="label label-info">PDF</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v70/rana17a/rana17a.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    S. Rana, C. Li, S. Gupta, <strong>V. Nguyen</strong>, S. Venkatesh <br>
	    <strong>High Dimensional Bayesian Optimization with Elastic Gaussian Process</strong><br>

        In           <i>Proceedings of the 34th International Conference on Machine Learning</i>, (<strong>ICML</strong>), pp 2883-2891,   2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICML17" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICML17" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">High Dimensional Bayesian Optimization with Elastic Gaussian Process</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization is an efficient way to optimize expensive black-box functions such as designing a new product with highest quality or hyperparameter tuning of a machine learning algorithm. However, it has a serious limitation when the parameter space is high-dimensional as Bayesian optimization crucially depends on solving a global optimization of a surrogate utility function in the same sized dimensions. The surrogate utility function, known commonly as acquisition function is a continuous function but can be extremely sharp at high dimension - having only a few peaks marooned in a large terrain of almost flat surface. Global optimization algorithms such as DIRECT are infeasible at higher dimensions and gradient-dependent methods cannot move if initialized in the flat terrain. We propose an algorithm that enables local gradient-dependent algorithms to move through the flat terrain by using a sequence of gross-to-finer Gaussian process priors on the objective function as we leverage two underlying facts - a) there exists a large enough length-scales for which the acquisition function can be made to have a significant gradient at any location in the parameter space, and b) the extrema of the consecutive acquisition functions are close although they are different only due to a small difference in the length-scales. Theoretical guarantees are provided and experiments clearly demonstrate the utility of the proposed method at high dimension using both benchmark test functions and real-world case studies.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="http://proceedings.mlr.press/v70/rana17a/rana17a.pdf" class="label label-info">PDF</a>

  </div>
</div>






<div class="media">
  <a class="pull-left thumbnail" href="http://static.ijcai.org/proceedings-2017/0355.pdf">
    <img src="./img/paper/ijcai2017.png" alt="">
  </a>
  <div class="media-body">
     <strong>V. Nguyen</strong>, D. Phung, T. Le, S. Venkatesh, H. Bui <br>
	    <strong>Discriminative Bayesian Nonparametric Clustering</strong><br>

        In           <i>Proceedings of International Joint Conference on Artificial Intelligence </i>, (<strong>IJCAI</strong>), pp 2550-2556,    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractIJCAI_Vu" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractIJCAI_Vu" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Discriminative Bayesian Nonparametric Clustering</h4>
            </div>
            <div class="modal-body">
			We propose a general framework for discriminative
Bayesian nonparametric clustering to promote
the inter-discrimination among the learned clusters
in a fully Bayesian nonparametric (BNP) manner.
Our method combines existing BNP clustering and
discriminative models by enforcing latent cluster
indices to be consistent with the predicted labels
resulted from probabilistic discriminative model.
This formulation results in a well-defined generative
process wherein we can use either logistic regression
or SVM for discrimination. Using the proposed
framework, we develop two novel discriminative
BNP variants: the discriminative Dirichlet
process mixtures, and the discriminative-state in-
finite HMMs for sequential data. We develop ef-
ficient data-augmentation Gibbs samplers for posterior
inference. Extensive experiments in image
clustering and dynamic location clustering demonstrate
that by encouraging discrimination between
induced clusters, our model enhances the quality of
clustering in comparison with the traditional generative
BNP models.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	  	          <a href="http://static.ijcai.org/proceedings-2017/0355.pdf" class="label label-info">PDF</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v63/nguyen93.html">
    <img src="./img/paper/acml2016.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. K. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>A Bayesian Nonparametric Approach for Multi-label Classification</strong><br>

        In           <i>Proceedings of The 8th Asian Conference on Machine Learning</i>, (<strong>ACML</strong>), pp 254-269,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractACML_BNMC" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractACML_BNMC" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">A Bayesian Nonparametric Approach for Multi-label Classification</h4>
            </div>
            <div class="modal-body">
			Many real-world applications require multi-label classification where multiple target labels
are assigned to each instance. In multi-label classification, there exist the intrinsic correlations
between the labels and features. These correlations are beneficial for multi-label
classification task since they reflect the coexistence of the input and output spaces that can
be exploited for prediction. Traditional classification methods have attempted to reveal
these correlations in different ways. However, existing methods demand expensive computation
complexity for finding such correlation structures. Furthermore, these approaches
can not identify the suitable number of label-feature correlation patterns. In this paper, we
propose a Bayesian nonparametric (BNP) framework for multi-label classification that can
automatically learn and exploit the unknown number of multi-label correlation. We utilize
the recent techniques in stochastic inference to derive the cheap (but efficient) posterior
inference algorithm for the model. In addition, our model can naturally exploit the useful
information from missing label samples. Furthermore, we extend the model to update parameters
in an online fashion that highlights the flexibility of our model against the existing
approaches. We compare our method with the state-of-the-art multi-label classification algorithms
on real-world datasets using both complete and missing label settings. Our model
achieves better classification accuracy while our running time is consistently much faster
than the baselines in an order of magnitude.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	          <a href="http://proceedings.mlr.press/v63/nguyen93.html" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ACML2016_BNMC" class="label label-success">Code</a>
		<a href="https://www.youtube.com/watch?v=-EE-I2IpQbo" class="label label-success">Youtube Demo</a>
		<a href="" class="label label-danger">Best Paper Runner Up Award</a>
		<a href="" class="label label-danger">Best Poster Award</a>

	  
  </div>
</div>




  
<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/7837957/">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Rana, S. K. Gupta, C. Li, S. Venkatesh <br>
	    <strong>Budgeted Batch Bayesian Optimization</strong><br>

        In           <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 1107-1112,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractB3O_ICDM" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractB3O_ICDM" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Budgeted Batch Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			Parameter settings profoundly impact the performance
of machine learning algorithms and laboratory experiments.
The classical trial-error methods are exponentially expensive
in large parameter spaces, and Bayesian optimization (BO)
offers an elegant alternative for global optimization of black box
functions. In situations where the functions can be evaluated
at multiple points simultaneously, batch Bayesian optimization
is used. Current batch BO approaches are restrictive in fixing
the number of evaluations per batch, and this can be wasteful
when the number of specified evaluations is larger than the
number of real maxima in the underlying acquisition function.
We present the budgeted batch Bayesian optimization (B3O) for
hyper-parameter tuning and experimental design - we identify
the appropriate batch size for each iteration in an elegant
way. In particular, we use the infinite Gaussian mixture model
(IGMM) for automatically identifying the number of peaks in
the underlying acquisition functions. We solve the intractability
of estimating the IGMM directly from the acquisition function
by formulating the batch generalized slice sampling to efficiently
draw samples from the acquisition function. We perform extensive
experiments for benchmark functions and two real world
applications - machine learning hyper-parameter tuning and
experimental design for alloy hardening. We show empirically
that the proposed B3O outperforms the existing fixed batch BO
approaches in finding the optimum whilst requiring a fewer
number of evaluations, thus saving cost and time.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	          <a href="http://ieeexplore.ieee.org/document/7837957/" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ICDM2016_B3O" class="label label-success">Code</a>
	  
  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/7837958/">
    <img src="./img/paper/ICDM2016_OLR.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, T. D. Nguyen, T. Le, S. Venkatesh, D. Phung.  <br>
	    <strong>One-pass Logistic Regression for Label-drift and Large-scale Classification on Distributed Systems </strong><br>

        In           <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 1113-1118,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractOLR_ICDM" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractOLR_ICDM" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">One-pass Logistic Regression for Label-drift and Large-scale Classification on Distributed Systems</h4>
            </div>
            <div class="modal-body">
			Logistic regression (LR) is at the cornerstone of classification. Its extension for multiclass classification is the
workhorse in industry, where a set of predefined classes is required. The model, however, fails to work in the case where
the class labels are not known in advance, a problem we term label-drift classification, in a similar spirit of the so-called conceptdrift
problem in the literature. Label-drift classification problem naturally occurs in many applications, especially in the context
of streaming and online settings where the incoming data may contain samples categorized with new classes that have not
been previously seen. Additionally, in the wave of big data, traditional LR methods may fail due to their expense of running
time and label-drift requirements. In this paper, we introduce a novel variant of LR, namely one-pass logistic regression (OLR)
to offer a principled treatment for large-scale and label-drift classifications. Our key contribution is the derivation of sufficient
statistic update for MAP estimation of Polya-Gamma augmentation for LR. Manipulating these sufficient statistics is convenient,
allowing our proposed method to efficiently perform the labeldrift classification under an online setting without retraining the
model from scratch. To handle large-scale classification for big data, we further extend our OLR to a distributed setting for
parallelization, termed sparkling OLR (Spark-OLR). We demonstrate the scalability of our proposed methods on large-scale
datasets with more than one hundred million data points. The experimental results show that the predictive performances of our
methods are comparable or better than those of state-of-the-art baselines whilst the execution time is much faster at an order of
magnitude. To measure the inherent trade-off between speed and accuracy, we propose quadrant visualization and quadrant score,
on which our proposed model outperforms other methods on all datasets. In addition, the OLR and Spark-OLR are invariant
to data shuffling and have no hyperparameter to tune that significantly benefits data practitioners and overcomes the curse
of big data cross-validation to select optimal hyperparameters.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	          <a href="http://ieeexplore.ieee.org/document/7837958/" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ICDM2016_OLR" class="label label-success">Code</a>
  </div>
</div>




  
<div class="media">
  <a class="pull-left thumbnail" href="http://jmlr.org/proceedings/papers/v51/le16.pdf">
    <img src="./img/paper/AISTATS2016.jpg" alt="">
  </a>
  <div class="media-body">
    T. Le, <strong>V. Nguyen</strong>, TD. Nguyen, D. Phung <br>
	    <strong>Nonparametric Budgeted Stochastic Gradient Descent</strong><br>

        In           <i>Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</i>, (<strong>AISTATS</strong>),  pp 654-572,    2016.<br>
				  
      <a data-toggle="modal" href="http://http://prada-research.net/~tienvu/#abstractabstractNonparametricBudgetedStochasticGradientDescent" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractabstractNonparametricBudgetedStochasticGradientDescent" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Nonparametric Budgeted Stochastic Gradient Descent</h4>
            </div>
            <div class="modal-body">
One of the most challenging problems in kernel online learning is to bound the model size. 
Budgeted kernel online learning addresses this issue by bounding the model size to a predefined budget. 
However, determining an appropriate value for such predefined budget is arduous. 
In this paper, we propose the Nonparametric Budgeted Stochastic Gradient Descent that allows the model size to automatically grow with data in a principled way. 
We provide theoretical analysis to show that our framework is guaranteed to converge for a large collection of loss functions 
(e.g. Hinge, Logistic, L2, L1, and ε-insensitive) which enables the proposed algorithm to perform both classification and regression tasks without hurting the ideal convergence rate O(1/T) 
of the standard Stochastic Gradient Descent. We validate our algorithm on the real-world datasets to consolidate the theoretical claims.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://jmlr.org/proceedings/papers/v51/le16.pdf" class="label label-info">PDF</a>
      <a href="https://github.com/ntienvu/NonparametricBudgetedSGD" class="label label-success">Code</a>
  </div>
</div>


<div class="media">

  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7456501&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7456501">
    <img src="./img/paper/PERCOM2016.jpg" alt="">
  </a>
  <div class="media-body">
    T. Nguyen, <strong>V. Nguyen</strong>, FD. Salim, D. Phung<br>
	    <strong>SECC: Simultaneous Extraction of Context and Community from Pervasive Signals</strong><br>
        In           <i>Proceedings of 2016 IEEE International Conference on Pervasive Computing and Communications</i>, (<strong>PERCOM</strong>), pp 1-9,  2016.<br>
				  
      <a data-toggle="modal" href="http://http://prada-research.net/~tienvu/#abstractSECC" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractSECC" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">SECC: Simultaneous Extraction of Context and Community from Pervasive Signals</h4>
            </div>
            <div class="modal-body">
Understanding user contexts and group structures plays a central role in pervasive computing. These contexts and community structures are complex to mine from data collected in the 
wild due to the unprecedented growth of data, noise, uncertainties and complexities. Typical existing approaches would first extract the latent patterns to explain the human dynamics 
or behaviors and then use them as the way to consistently formulate numerical representations for community detection, often via a clustering method. While being able to capture highorder 
and complex representations, these two steps are performed separately. More importantly, they face a fundamental difficulty in determining the correct number of latent patterns and communities. 
This paper presents an approach that seamlessly addresses these challenges to simultaneously discover latent patterns and communities in a unified Bayesian nonparametric framework. 
Our Simultaneous Extraction of Context and Community (SECC) model roots in the nested Dirichlet process theory which allows nested structure to be built to explain data at multiple levels. 
We demonstrate our framework on three public datasets where the advantages of the proposed approach are validated.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7456501&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7456501" class="label label-info">PDF</a>
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://jmlr.org/proceedings/papers/v32/nguyenb14.html">
    <img src="./img/paper/ICML2014.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>V. Nguyen</strong>, D. Phung, L. Nguyen, S. Venkatesh and H. Bui   <br>
       <strong>Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts.</strong><br>

        In           <i>Proceedings of The 31st International Conference on Machine Learning</i> (<strong>ICML</strong>), pp. 288–296, 2014.<br>
				  
      <a data-toggle="modal" href="http://http://prada-research.net/~tienvu/#abstractICML14" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICML14" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts</h4>
            </div>
            <div class="modal-body">
We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents 
and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context 
observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: 
integrating out all contents results in the DPM over contexts, whereas integrating out group-speciﬁc contexts results in the nDP mixture over content variables. We provide a Polya-urn view of 
the model and an efﬁcient collapsed Gibbs inference procedure. 
Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://jmlr.org/proceedings/papers/v32/nguyenb14.html" class="label label-info">PDF</a>

  </div>
</div>



		  <br>
	  <br> 
	  <br>
	  <br>
	  <br>
	  <br>
	  <br>
	  
<blockquote>
        <a href="http://s11.flagcounter.com/more/XfDO"><img
src="http://s11.flagcounter.com/count/XfDO/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"
            alt="free counters" border="0"></a>
        <!-- hitwebcounter Code START --> <a
          href="http://www.hitwebcounter.com/counterresources.php"
          target="_blank"> <img
src="http://hitwebcounter.com/counter/counter.php?page=4530260&amp;style=0025&amp;nbdigits=5&amp;type=page&amp;initCount=0"
            title="visitors base of the site" alt="visitors base of the
            site" border="0"> </a><br>
        <!-- hitwebcounter.com --><a
          href="http://www.hitwebcounter.com/" title="Web Counter"
          target="_blank" style="font-family: Arial, Helvetica,
          sans-serif; font-size: 15px; color: #908E86; text-decoration:
          none ;"><code>Web Counter</code> </a> </blockquote>

      <footer class="text-center text-muted">
        <hr>
        Last updated Jan 05, 2018.<br>
	Based on the code of 
        <a href="https://github.com/alopez/alopez.github.com">Adam Lopez</a>.<br>
        Created with 
        <a href="http://git-scm.com/">git</a>,
        <a href="http://jekyllrb.com/">jekyll</a>,
        <a href="http://getbootstrap.com/">bootstrap</a>,
        and <a href="http://www.vim.org/">vim</a>.<br> 
        <br><br>
      </footer>
    </div>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
         m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-46770577-1', 'jhu.edu');
      ga('send', 'pageview');
    </script>
  


</body></html>