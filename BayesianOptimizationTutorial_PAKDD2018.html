<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<!-- saved from url=(0049)https://mslab.csie.ntu.edu.tw/www15tut/#reference -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Advances in Bayesian Optimization</title>

<link rel="stylesheet" type="text/css" href="./BO_Tutorial_PAKDD2018_files/style.css">
<style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style></head>

<body>
<div id="doc">

<div id="right-column">

<div class="call-out-green">
<div class="subpic"><a href="http://prada-research.net/pakdd18/" style="color:#005599">PAKDD 2018 Tutorial</a></div>
<div class="pic">Advances in Bayesian Optimization</div>
</div>

<div id="navigation">
<ul>
<li><a href="#description" target="_self">Description</a></li>
<li><a href="#outline" target="_self">Outline</a></li>
<li><a href="#slides" target="_self">Slides</a></li>
<li><a href="#require" target="_self">Prerequists</a></li>
<li><a href="#presenter" target="_self">Presenters</a></li>
<li><a href="#reference" target="_self">References</a></li>
</ul>
</div>

<h1><a name="description"></a>Description</h1>
<p>Bayesian optimization has emerged as an exciting sub-field of machine learning and artificial intelligence that is concerned with optimization using probabilistic methods. 
Systems implementing Bayesian optimization techniques have been successfully used to solve difficult problems in a diverse set of applications, including automatic tuning of machine learning algorithms, experimental designs, and many other systems. 
Several recent advances in the methodologies and theory underlying Bayesian optimization have extended the framework to new applications and provided greater insights into the behavior of 
these algorithms. Bayesian optimization is now increasingly being used in industrial settings, providing new and interesting challenges that require new algorithms and theoretical insights.

Therefore, I think having a tutorial on Bayesian optimization for PAKDD audience is timely, useful, and practical for both academia and industries to know the recent advances on Bayesian optimization in a systematic manner. 
The topics of this tutorial consists of two main parts. In the first part, I will go into detail the Bayesian optimization in the standard and simple setting. In the second part, I will present the current advances in Bayesian optimization including
(1) batch Bayesian optimization, (2) Bayesian optimization in unknown search space and (3) high dimensional Bayesian optimization.
In the end of the talk, I also outline the possible future research directions in Bayesian optimization.</p>

<p><b>Part I: Bayesian Optimization.</b><br>
Bayesian optimization is a sequential model-based approach to solving global optimization problem of black-box functions. By black-box function, we will assume
the function f has no simple closed form, but can be evaluated at any arbitrary query point x in the domain. In particular,
the Bayesian optimization framework has two key ingredients. The first ingredient is a probabilistic surrogate model, which consists of a prior distribution that
captures our beliefs about the behavior of the unknown objective function and an observation model that describes the data generation mechanism. The second ingredient is a
loss function that describes how optimal a sequence of queries are; in practice, these loss functions often take the form of regret, either simple or cumulative. Ideally, the
expected loss is then minimized to select an optimal sequence of queries. After observing the output of each query of the objective, the prior is updated to produce a
more informative posterior distribution over the space of objective functions.

</p>

<p><b>Part II.1: Advances in Bayesian Optimization - Batch Bayesian Optimization.</b><br>
Standard Bayesian optimization approaches only allow the exploration
of the parameter space to occur sequentially. Often, it is desirable to simultaneously
propose batches of parameter values
to explore. This is particularly the case when
large parallel processing facilities are available.
These could either be computational
or physical facets of the process being optimized.
Batch methods, however, require the
modeling of the interaction between the different
evaluations in the batch, which can be
expensive in complex scenarios. In this section, I will summarize the recent batch Bayesian optimization models. I will provide the strengths and weaknesses of each approach.</p>

<p><b>Part II.2: Advances in Bayesian Optimization - Bayesian Optimization in Weakly Specified Search Space</b><br>
Existing Bayesian optimization approaches are restricted to
a pre-defined and fixed space of search wherein it is assumed
to contain the global optimum. Unfortunately, setting these
regions so that it encapsulates the global optimum is nontrivial
and often done arbitrarily. The main reason is that in
many situations specifying a search space X is hard for a
new problem or where domain knowledge is limited. This
remain a key challenge that hinders us from getting the best
performance for global optimization. Setting these regions arbitrarily can lead to
inefficient optimization - if a space is too large, we can miss the
optimum with a limited budget, on the other hand, if a space is
too small, it may not contain the optimum point that we want to
get. The unknown search space problem is intractable to solve in
practice. In this section, I will present the latest Bayesian optimization approaches to tackle this unknown search space issues.
</p>

<p><b>Part II.3: Advances in Bayesian Optimization - High Dimensional Bayesian Optimization</b><br>
Standard Bayesian optimization is limited to about 10 dimensions. Scaling BO methods to handle functions in high dimension presents two main challenges. Firstly, the number
of observations required by the GP grows exponentially as input dimensions increase. This implies more experimental
evaluations are required, often expensive and infeasible in real applications. Secondly, global optimization for high dimensional
acquisition functions is intrinsically a hard problem and can be prohibitively expensive to be feasible. I will discuss recent advances in Bayesian optimization techniques for high dimensional settings.</p>


<h1><a name="outline"></a>Tutorial Outline</h1>
<p><b>Tutorial Outline and Motivation to Bayesian Optimization (15 min)</b></p>
<ul>
<li>Machine Learning Hyper-parameter Tuning and Experimental Design as Black-box Function <span style="color:#006699;">[29][14]</span></li>
<li>Bayesian Optimization for Optimizing a Black-box Function  <span style="color:#006699;">[29][14]</span></li>
</ul>

<p><b>Part I. Bayesian Optimization (75 mins)</b></p>
<ul>
<li>Bayesian Optimization [10 mins] <span style="color:#006699;">[14][27][28]</span></li>
<li>Gaussian Processes [10 mins] <span style="color:#006699;">[7][15]</span></li>
<li>Acquisition Function [10 mins] <span style="color:#006699;">[1][13][16][17][18][23]</span></li>
<li>Illustration of Bayesian Optimization [10 mins] <span style="color:#006699;"></span></li>
<li>Convergence Analysis in Bayesian Optimization [10 mins] <span style="color:#006699;">[1][13]</span></li>
<li>Applications of Bayesian Optimization [15 mins]<span style="color:#006699;">[4][5][11][12][26]</span></li>
<li>Question and Answer [10 mins]</li>
</ul>

<p><b>Part II.1. Advances in Bayesian Optimization - Batch Bayesian Optimization (30 min)</b></p>
<ul>
<li>Introduction and Problem Statements [5 mins]</li>
<li>Peak Suppression Approaches: Constant Liar, GP-BUCB, GP-UCB-PE, Local Penalization [10 mins]<span style="color:#006699;">[19][20][21][22][27]</span></li>
<li>Budgeted Batch Bayesian Optimization for Unknown Batch Size [5 mins]<span style="color:#006699;">[6]</span></li>
<li>Creating More Peaks with Local Expected Improvement[5 mins]<span style="color:#006699;">[9]</span></li>
<li>Thompson Sampling for Batch Bayesian Optimization [5 mins]<span style="color:#006699;">[29]</span></li>
</ul>


</ul>
<p><b>Part II.2. Advances in Bayesian Optimization - Bayesian Optimization in Unknown Search Space (30 min)</b></p>
<ul>
<li>Introduction and Problem Statements [5 mins]</li>
<li>Volume Doubling and Regularization Approach [5 mins]<span style="color:#006699;">[14][24]</span></li>
<li>Bayesian Optimization in Weakly Specified Search Space [15 mins]<span style="color:#006699;">[2]</span></li>
<li>Applications [5 mins]<span style="color:#006699;">[2][5][12]</span></li>
</ul>

</ul>
<p><b>Part II.3. Advances in Bayesian Optimization - High Dimensional Bayesian Optimization (15 min)</b></p>
<ul>
<li>Introduction and Problem Statements</li>
<li>Existing Approaches in High Dimensional Bayesian Optimization.[5 mins]<span style="color:#006699;">[14]</span></li>
<li>High Dimensional Bayesian Optimization with Elastic Gaussian Process. [5 mins]<span style="color:#006699;">[3][10]</span></li>
<li>High Dimensional Bayesian Optimization Using Dropout. [5 mins]<span style="color:#006699;">[8]</span></li>
</ul>

<p><b>Future Research Directions and Q&A (15 mins)</b></p>
<ul>
<li>Future Research Directions [5 mins]</li>
<li>Question and Answer [10 mins]</li>

</ul>

<h1><a name="slides"></a>Tutorial Slides</h1>
<p>Tutorial slides can be downloaded here: <a href="https://www.dropbox.com/">download</a></p>

<h1><a name="require"></a>Prerequisites</h1>
<p>We do not require the audiences to have strong background knowledge on Bayesian modelling. However, we expect the audience already understand some basic concepts and terminologies on artificial intelligence, data mining, and machine learning.</p>

<h1><a name="require"></a>Description of the previous versions of tutorial</h1>
<p>Different parts of the tutorial are presented by the speaker and his colleagues at ICDM 2016, NIPSW 2016, IJCAI 2017, ICML 2017, ACML 2017, ICDM 2017, respectively.</p>


<h1><a name="presenter"></a>Presenters</h1>
<p><a href="https://ntienvu.github.io/index.html">Dr Vu Nguyen</a> is currently an Associate Research Fellow at Center for Pattern Recognition and Data Analytics (PRaDA) at Deakin University, Australia. 
He received his Ph.D. degrees from Deakin University, in 2015 under the supervision of Prof. Dinh Phung and Prof. Svetha Venkatesh. 
His research interests include Bayesian Nonparametrics, Bayesian Optimization. 
His international recognition includes Travel Grant Machine Learning Summer School 2011, Selected Participants for Heidelberg Laurate Forum 2015, Finalist Awards - Track 1 and Track 5 ICPR 2015, 
Best Paper Runner up Award and Best Poster Award - ACML 2016, Vice Chancellor Award for Outstanding Contribution - Deakin University 2017.
He gains expertise on Machine Learning and Bayesian Optimization with 20 papers published in premier venues, including ICDM, ACML, ICML, IJCAI, AISTATS, and NIPS.</p>

<h1><a name="reference"></a>References</h1>
<ol>
<li><b>V. Nguyen</b>, S. Gupta, S. Rana, C. Li, S. Venkatesh. 
Regret for Expected Improvement over the Best-Observed Value and Stopping Condition.
In Proceedings of The 9th Asian Conference on Machine Learning (<i>ACML</i>), 2017.</li>
<li><b>V. Nguyen</b>, S. Gupta, S. Rana, C. Li, S. Venkatesh. 
Bayesian Optimization in Weakly Specified Search Space.
In Proceedings of the IEEE International Conference on Data Mining (<i>ICDM</i>), 2017.</li>
<li>S. Rana, C. Li, S. Gupta, <b>V. Nguyen</b>, S. Venkatesh.
High Dimensional Bayesian Optimization with Elastic Gaussian Process.
In Proceedings of the 34th International Conference on Machine Learning (<i>ICML</i>), pp 2883-2891, 2017.</li>

<li>K Deane, <b>V. Nguyen</b>, S Rana, S Gupta, S Venkatesh, PG. Sanders.
Utilization of Bayesian Optimization and KWN Modeling for Increased Efficiency of Al-Sc Precipitation Strengthening.
Metallurgical and Materials Transactions B (under review)</li>

<li>P. Sanders, S. Rana, J. Licavoli, S. Gupta, <b>V. Nguyen</b>, S. Venkatesh. 
Bayesian Optimization of Superalloy Design.
4th World Congress on Integrated Computational Materials Engineering 2017.</li>

<li><b>V. Nguyen</b>, S. Rana, S. K. Gupta, C. Li, S. Venkatesh. 
Budgeted Batch Bayesian Optimization.
In Proceedings of the IEEE International Conference on Data Mining (<i>ICDM</i>), pp 1107-1112, 2016.</li>

<li>T. Le, K. Nguyen, <b>V. Nguyen</b>, T. D. Nguyen, D. Phung. 
GoGP: Fast Online Regression with Gaussian Processes. 
In Proceedings of the IEEE International Conference on Data Mining (<i>ICDM</i>), 2017.</li>

<li>C. Li, S. Gupta, S. Rana, <b>V. Nguyen</b>, S. Venkatesh, A. Shilton. 
High Dimensional Bayesian Optimization Using Dropout.
In Proceedings of International Joint Conference on Artificial Intelligence (<i>IJCAI</i>), pp 2096-2102, 2017.</li>

<li><b>V. Nguyen</b>, S. K. Gupta, S. Rana, C. Li, S. Venkatesh.
Think Globally, Act Locally: a Local Strategy for Bayesian Optimization.
In NIPS Workshop on Bayesian Optimization (<i>NIPSW</i>), 2016.</li>

<li>C. Li, S. K. Gupta, S. Rana, <b>V. Nguyen</b>, S. Venkatesh. 
High Dimensional Bayesian Optimization with Elastic Gaussian Process.
In NIPS Workshop on Bayesian Optimization (<i>NIPSW</i>), 2016.</li>

<li>T. D. Nguyen, S. K. Gupta, S. Rana, <b>V. Nguyen</b>, S. Venkatesh, K. Deane, P. Sanders.
Cascade Bayesian Optimization.
In Proceedings The 29th Australasian Joint Conference on Artificial Intelligence (<i>AusAI</i>), pp 268-280, 2016.</li>

<li><b>V. Nguyen</b>, S. Gupta, S. Rana, C. Li, & S. Venkatesh. A Bayesian Nonparametric Approach for Multi-label Classification. In Asian Conference on Machine Learning (<i>ACML</i>), pp. 254-269, 2016.</li>


<li>Srinivas, N., Krause, A., Kakade, S., & Seeger, M. Gaussian process optimization in the
bandit setting: No regret and experimental design. In Proceedings of the 27th International
Conference on Machine Learning (<i>ICML</i>), pp. 1015–1022, 2010.</li>

<li>Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & de Freitas, N. Taking the human out
of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1), 148–175, 2016.</li>

<li>Rasmussen, C. E. Gaussian processes for machine learning, 2006.</li>

<li>Mockus, J., Tiesis, V., & Zilinskas, A. The application of bayesian methods for seeking the
extremum. Towards global optimization, 2(117-129), 2, 1978.</li>

<li>Hernandez-Lobato, J. M., Hoffman, M. W., & Ghahramani, Z. Predictive entropy search
for efficient global optimization of black-box functions. In Advances in Neural Information
Processing Systems (<i>NIPS</i>), pp. 918–926, 2014.</li>

<li>Hennig, P., & Schuler, C. J. Entropy search for information-efficient global optimization.
Journal of Machine Learning Research (<i>JMLR</i>), 13, 1809–1837, 2012.
</li>

<li>Gonzalez, J., Dai, Z., Hennig, P., & Lawrence, N. D. Batch bayesian optimization via local
penalization. In Proceedings of the 19th International Conference on Artificial Intelligence
and Statistics (<i>AISTATS</i>), pp. 648–657, 2016.
</li>

<li>Ginsbourger, D., Le Riche, R., & Carraro, L. A multi-points criterion for deterministic
parallel global optimization based on gaussian processes, 2008. </li>

<li>Desautels, T., Krause, A., & Burdick, J. W. Parallelizing exploration-exploitation tradeoffs
in gaussian process bandit optimization. The Journal of Machine Learning Research (<i>JMLR</i>), 15(1),
3873–3923, 2014.</li>

<li>Contal, E., Buffoni, D., Robicquet, A., & Vayatis, N. Parallel gaussian process optimization
with upper confidence bound and pure exploration. In Machine Learning and Knowledge
Discovery in Databases, pp. 225–240. Springer, 2013.</li>


<li>Jones, D. R., Perttunen, C. D., & Stuckman, B. E. Lipschitzian optimization without the
lipschitz constant. Journal of Optimization Theory and Applications, 79(1), 157–181, 1993.</li>

<li>Shahriari, B., Bouchard-Côté, A., & Freitas, N. Unbounded Bayesian optimization via regularization. In Artificial Intelligence and Statistics (<i>AISTATS</i>), pp. 1168-1176, 2016.</li>

<li>Rahimi, A. and Recht, B., Random features for large-scale kernel machines. In Advances in neural information processing systems (<i>NIPS</i>), pp. 1177-1184, 2008.</li>

<li>Li, C., de Celis Leal, D.R., Rana, S., Gupta, S., Sutti, A., Greenhill, S., Slezak, T., Height, M. and Venkatesh, S.. Rapid Bayesian optimisation for synthesis of short polymer fiber materials. Scientific reports, 7(1), p.5683, 2017.</li>

<li>Snoek, J., Larochelle, H., & Adams, R. P. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems (<i>NIPS</i>), pp. 2951-2959, 2012.
</li>

<li>Brochu, E., Cora, V. M., & De Freitas, N. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv:1012.2599, 2010.

<li> Hernández-Lobato, J. M., Requeima, J., Pyzer-Knapp, E. O., & Aspuru-Guzik, A. Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space. In Proceedings of the 27th International
Conference on Machine Learning (<i>ICML</i>) 2017. </li>
</ol>
<p>&nbsp;</p>

<!-- Site Meter -->
<script type="text/javascript" src="http://s51.sitemeter.com/js/counter.js?site=s51pakdd13sss">
</script>
<noscript>
&lt;a href="http://s51.sitemeter.com/stats.asp?site=s51pakdd13sss" target="_top"&gt;
&lt;img src="http://s51.sitemeter.com/meter.asp?site=s51pakdd13sss" alt="Site Meter" border="0"/&gt;&lt;/a&gt;
</noscript>
<!-- Copyright (c)2009 Site Meter -->



</div>

</div>


</body></html>