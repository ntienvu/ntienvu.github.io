#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{mathptmx}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.7cm
\topmargin 1.3cm
\rightmargin 1.7cm
\bottommargin 1.3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
date{}
\end_layout

\end_inset


\end_layout

\begin_layout Title
Recent Advances in Bayesian Optimization
\end_layout

\begin_layout Author
Dr Vu Nguyen
\begin_inset Newline newline
\end_inset

vu@robots.ox.ac.uk
\begin_inset Newline newline
\end_inset

University of Oxford, UK
\end_layout

\begin_layout Standard
\begin_inset VSpace -20pt
\end_inset


\end_layout

\begin_layout Abstract
Bayesian optimization (BO) has emerged as an exciting sub-field of machine
 learning and artificial intelligence that is concerned with optimization
 using probabilistic methods.
 Systems implementing BO techniques have been successfully used to solve
 difficult problems in a diverse set of applications, including automatic
 tuning of machine learning algorithms, experimental designs, and many other
 systems.
 Several recent advances in the methodologies and theory underlying BO have
 extended the framework to new applications and provided greater insights
 into the behavior of these algorithms.
 Bayesian optimization is now increasingly being used in industrial settings,
 providing new and interesting challenges that require new algorithms and
 theoretical insights.
 Therefore, I think having a tutorial on Bayesian optimization for ACML
 audience is timely, useful, and practical for both academia and industries
 to know the recent advances on Bayesian optimization in a systematic manner.
 The topics of this tutorial consists of two main parts.
 In the first part, I will go into detail the BO in the standard setting.
 In the second part, I will present the current advances in Bayesian optimizatio
n including (1) batch BO, (2) high dimensional BO and (3) mixed categorical-cont
inuous BO.
 In the end of the talk, I also outline the possible future research directions
 in Bayesian optimization.
\end_layout

\begin_layout Paragraph
Tutorial Website:
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
target "https://ntienvu.github.io/BayesianOptimizationTutorial_ACML2020.html"
literal "false"

\end_inset


\end_layout

\begin_layout Section
Goals and Objectives
\end_layout

\begin_layout Paragraph
Specific goals and objectives
\end_layout

\begin_layout Standard
The goal of this tutorial is offering ACML audience a timely, useful, and
 practical introduction of Bayesian optimization in a systematic manner.
 In particular, my aim is to introduce the techniques, applications and
 future research directions of BO.
 In addition, I provide a broad summary of recent advances in batch BO,
 in unknown search space and in high dimension settings.
\end_layout

\begin_layout Paragraph
Why is the topic important/interesting to ACML audience?
\end_layout

\begin_layout Standard
There is proliferation of machine learning and data mining algorithms which
 hyper-parameters tuning is needed.
 Grid search and random search are two popular approaches for finding the
 best hyper-parameters.
 Can we do better than grid search and random search? This tutorial provides
 insides on how BO can be used to tune the model parameters in the fewest
 iteration with theoretical guarantee.
 Bayesian optimization will lift up the performances of all machine learning
 and data mining algorithms which are sensitive to the choice of hyper-parameter
s.
 
\emph on
As a result, all ACML audience who are dealing with hyper-parameter tuning
 for their algorithms will be greatly benefited.
\end_layout

\begin_layout Paragraph
What is the expected background of the audience?
\end_layout

\begin_layout Standard
I do not require the audiences to have strong background knowledge on Bayesian
 modeling.
 However, we expect the audience already understand some basic concepts
 and terminologies on artificial intelligence, data mining, and machine
 learning.
\end_layout

\begin_layout Paragraph
Description of the previous versions of tutorial
\end_layout

\begin_layout Itemize
A shorter version of this tutorial has been delivered at University of Twente
 (The Netherland), University of Liverpool (UK), University of Glasgow (UK)
 in June 2020, Amazon Research in July 2020.
\end_layout

\begin_layout Itemize
The GP part and the demo for BO have been taught in class lectures by Dr
 Vu Nguyen at University of Oxford in Oct 2019.
\end_layout

\begin_layout Itemize
Different parts of the tutorial are presented by the speaker and his colleagues
 at ICDM 2016, NIPSW 2016, IJCAI 2017, ICML 2017, ACML 2017, ICDM 2017,
 ICDM 2018, NeurIPS 2018, ICDM 2019, ICML 2020 respectively.
\end_layout

\begin_layout Section
Outline including a short summary of every section 
\end_layout

\begin_layout Subsection
Tutorial Outline and Motivation to Bayesian Optimization [5 min]
\end_layout

\begin_layout Standard
I will start with the problem of machine learning hyper-parameter tuning
 and experimental design which can be seen as the black-box function.
 Then, I will give a brief introduction to optimize these black-box functions
 using Bayesian optimization.
\end_layout

\begin_layout Itemize
Machine Learning Hyper-parameter Tuning and Experimental Design as Black-box
 Function
\end_layout

\begin_layout Itemize
Bayesian Optimization for Optimizing a Black-box Function 
\end_layout

\begin_layout Subsection
Part I.
 Bayesian Optimization [55 min]
\end_layout

\begin_layout Standard
Bayesian optimization is a sequential model-based approach to solving global
 optimization problem of black-box functions.
 By black-box function, we assume that the function 
\begin_inset Formula $f$
\end_inset

 has no simple closed form, but can be evaluated at any arbitrary query
 point 
\begin_inset Formula $x$
\end_inset

 in the domain.
 In particular, the BO framework has two key ingredients.
 The first ingredient is a probabilistic surrogate model, which consists
 of a prior distribution that captures our beliefs about the behavior of
 the unknown objective function and an observation model that describes
 the data generation mechanism.
 The second ingredient is a loss function that describes how optimal a sequence
 of queries are; in practice, these loss functions often take the form of
 regret, either simple or cumulative.
 Ideally, the expected loss is then minimized to select an optimal sequence
 of queries.
 After observing the output of each query of the objective, the prior is
 updated to produce a more informative posterior distribution over the space
 of objective functions.
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "52col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Bayesian Optimization [10 mins]
\end_layout

\begin_layout Itemize
Gaussian Processes and acquisition function [5 mins]
\end_layout

\begin_layout Itemize
Illustration of Bayesian Optimization [5 mins]
\end_layout

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "44col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Convergence Analysis in BO [5 mins]
\end_layout

\begin_layout Itemize
Applications of Bayesian Optimization [10 mins]
\end_layout

\begin_layout Itemize
Question and Answer [10 mins]
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Part II.
 Recent Advances in Bayesian Optimization
\end_layout

\begin_layout Subsubsection
Batch Bayesian Optimization [15 min]
\end_layout

\begin_layout Standard
Standard BO approaches allows the exploration of the parameter space to
 occur sequentially.
 Often, it is desirable to simultaneously propose batches of parameter values
 to explore.
 This is particularly the case when large parallel processing facilities
 are available.
 These could either be computational or physical facets of the process being
 optimized.
 Batch methods, however, require the modeling of the interaction between
 the different evaluations in the batch, which can be expensive in complex
 scenarios.
 In this section, I will summarize the recent batch BO models.
 I will provide the strengths and weaknesses of each approach.
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "49col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Introduction and Problem Statements [3 mins] 
\end_layout

\begin_layout Itemize
Peak Suppression Approaches [3 mins]
\end_layout

\begin_layout Itemize
Budgeted Batch BO for Unknown Batch Size [3 mins]
\end_layout

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Thompson Sampling for Batch BO [3 mins]
\end_layout

\begin_layout Itemize
Asynchronous Batch BO [3 mins]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Subsubsection
Bayesian Optimization in Unknown Search Space [25 min]
\end_layout

\begin_layout Plain Layout
Existing Bayesian optimization approaches are restricted to a pre-defined
 and fixed space of search wherein it is assumed to contain the global optimum.
 Unfortunately, setting these regions so that it encapsulates the global
 optimum is nontrivial and often done arbitrarily.
 The main reason is that in many situations specifying a search space 
\begin_inset Formula $\mathcal{X}$
\end_inset

 is hard for a new problem or where domain knowledge is limited.
 This remain a key challenge that hinders us from getting the best performance
 for global optimization.
 Setting these regions arbitrarily can lead to inefficient optimization
 - if a space is too large, we can miss the optimum with a limited budget,
 on the other hand, if a space is too small, it may not contain the optimum
 point that we want to get.
 The unknown search space problem is intractable to solve in practice.
 In this section, I will present the latest Bayesian optimization approaches
 to tackle this unknown search space issues.
\end_layout

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "49col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Introduction and Problem Statements [5 min]
\end_layout

\begin_layout Itemize
Volume Doubling and Regularization Approach [5 min]
\end_layout

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "49col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
BO in Weakly Specified Search Space [10 min]
\end_layout

\begin_layout Itemize
Applications [5 min]
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
High Dimensional Bayesian Optimization [15 min]
\end_layout

\begin_layout Standard
Existing BO is limited to about 10 dimensions.
 Scaling BO methods to handle functions in high dimension presents two main
 challenges.
 Firstly, the number of observations required by the GP grows exponentially
 as input dimensions increase.
 This implies more experimental evaluations are required, often expensive
 and infeasible in real applications.
 Secondly, global optimization for high dimensional acquisition functions
 is intrinsically a hard problem and can be prohibitively expensive to be
 feasible.
 I will discuss recent advances in Bayesian optimization techniques for
 high dimensional settings.
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "48col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Introduction and Problem Statements [3 min]
\end_layout

\begin_layout Itemize
Existing Approaches in High Dimensional BO [3 min]
\end_layout

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "49col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
High dim BO with Elastic GP [3 min]
\end_layout

\begin_layout Itemize
High dim BO using Dropout [3 min]
\end_layout

\begin_layout Itemize
High dim BO using Local Optimization.
 [3 mins]
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Mixed Categorical-Continuous Bayesian Optimization [20 min]
\end_layout

\begin_layout Standard
Real-world optimization problems are typically of mixed-variable nature,
 involving both continuous and categorical input variables.
 For example, tuning the hyperparameters of a deep neural network involves
 both continuous variables, e.g., learning rate and momentum, and categorical
 ones, e.g., optimizer types, activation type.
 Having a mixture of categorical and continuous variables presents unique
 challenges.
 If some inputs are categorical variables, as opposed to continuous, then
 the common assumption that the BO acquisition function is differentiable
 and continuous over the input space, which allows the acquisition function
 to be efficiently optimized, is no longer valid.
 I will discuss recent advances in BO for mixed categorical-continuous settings.
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "48col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Introduction and Problem Statements
\end_layout

\begin_layout Itemize
Existing Approaches in High Dimensional BO [5 min]
\end_layout

\begin_layout Itemize
Multi-armed Bandits [5 mins]
\end_layout

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "49col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Categorical-specific Continuous Optimization.
 [5 mins]
\end_layout

\begin_layout Itemize
Categorical Continuous Optimization.
 [5 mins]
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Future Research Directions and Q&A [15 min]
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "49col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Future Research Directions [5 min] 
\end_layout

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "49col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Itemize
Question and Answer [10 min]
\end_layout

\end_inset


\end_layout

\begin_layout Section
Presenter Biography
\end_layout

\begin_layout Standard
Dr Vu Nguyen is currently a Senior Research Associate at a Machine Learning
 Research Group at University of Oxford.
 He is working with Professor Michael Osborne and Professor Andrew Briggs
 on a machine learning project for tuning quantum devices using Bayesian
 optimization and deep reinforcement learning.
 Previously he was working as a Research Scientist at a Credit AI in Melbourne
 and was a postdoctoral researcher at Deakin University where he obtained
 his PhD in 2015.
 He published regularly at top venues in machine learning.
 He was the recipient of ACML 2016 best paper award, IEEE ICDM 2017 best
 papers and one of the 200 young researchers world-wide for attending Heidelberg
 Laureate Forum 2015.
 He gains expertise on Bayesian Machine Learning and Bayesian Optimization
 with 20 papers published in premier venues, including ICML, NeurIPS, ICDM,
 IJCAI, AISTATS and ACML.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "D:/Dropbox/03.Research/vunguyen"
options "plain"

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
