<!DOCTYPE html>
<!-- saved from url=(0050)http://people.eng.unimelb.edu.au/tcohn/papers.html -->
<html class="gr__people_eng_unimelb_edu_au"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>Vu Nguyen | Publications</title>

    <link href="./css/bootstrap.min.css" rel="stylesheet" media="screen">
    <link href="./css/bootstrap-glyphicons.css" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- MathJax -->
    <script async="" src="./js/analytics.js"></script><script type="text/javascript" src="./js/MathJax.js">
    </script>

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  <style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style></head>
  <body data-gr-c-s-loaded="true"><div id="MathJax_Message" style="display: none;"></div>
    <!-- JavaScript plugins (requires jQuery) -->
    <script src="./js/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./js/bootstrap.min.js"></script>

    <div class="container">
      <div class="row">
        <div class="col-sm-2">
          <br><br> 
<img src="./img/person/Vu_2016_Avatar.jpg" class="img-responsive" alt="bruge">
<ul class="nav navbar-inverse">
  <li>
    <a href="index.html">Home</a>
  </li>
  <li>
    <a href="publications.html">Publications</a>
  </li>
    <li>
    <a href="CV.html">CV</a>
  </li>

</ul>


        </div>
        <div class="col-sm-10">
          <div class="page-header">
		  
  <h3>Papers</h3>
</div>

<h4>Papers</h4>


<div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/rsz_3dprinting.jpg" alt="">
  </a>
  <div class="media-body">
  	C. Li, S. Rana, S. Gupta, <strong>V. Nguyen</strong>, S. Venkatesh, A. Sutti, D. Rubin, T. Slezak, M. Height, M. Mohammed, and I. Gibson <br>
	    <strong>Accelerating Experimental Design by Incorporating Experimenter Hunches</strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>), 2018.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICDM18" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICDM18" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Accelerating Experimental Design by Incorporating Experimenter Hunches</h4>
            </div>
            <div class="modal-body">
			
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 

	  
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
  	J. Berk, <strong>V. Nguyen</strong>, S. Gupta, S. Rana, S. Venkatesh <br>
	    <strong>Exploration Enhanced Expected Improvement for Bayesian Optimization</strong><br>

         <i>European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</i>, (<strong>ECML-PKDD</strong>), 2018.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractECML18" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractECML18" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Exploration Enhanced Expected Improvement for Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 

	  
  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
  	X. Zhang, W. Li, <strong>V. Nguyen</strong>, F. Zhuang, H. Xiong, S. Lu <br>
	    <strong>Label-Sensitive Task Grouping by Bayesian Nonparametric Approach for Multi-Task Multi-Label Learning</strong><br>

         <i>International Joint Conference on Artificial Intelligence</i>, (<strong>IJCAI</strong>),    2018.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractIJCAI18" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractIJCAI18" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Label-Sensitive Task Grouping by Bayesian Nonparametric Approach for Multi-Task Multi-Label Learning</h4>
            </div>
            <div class="modal-body">
			
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 

	  
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="https://bayesopt.github.io/papers/2017/13.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Filtering Bayesian Optimization Approach in Weakly Specified Search Space</strong><br>

         <i>Knowledge and Information Systems</i>, (KAIS),    2018.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractKAIS18_BO" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractKAIS18_BO" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Filtering Bayesian Optimization Approach in Weakly Specified Search Space</h4>
            </div>
            <div class="modal-body">
			
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 

	  
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="https://bayesopt.github.io/papers/2017/13.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    T. Le, K. Nguyen, <strong>V. Nguyen</strong>, T. D. Nguyen, D. Phung <br>
	    <strong>GoGP: Scalable Geometric-based Gaussian Process for Online Regression</strong><br>

         <i>Knowledge and Information Systems</i>, (KAIS),    2018.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractKAIS18_GOGP" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractKAIS18_GOGP" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">GoGP: Scalable Geometric-based Gaussian Process for Online Regression</h4>
            </div>
            <div class="modal-body">
			
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 

	  
  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="https://bayesopt.github.io/papers/2017/13.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Predictive Variance Reduction Search</strong><br>

         <i>NIPS Workshop on Bayesian Optimization</i>, (<strong>NIPSW</strong>),    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractNIPS17_PVRS" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractNIPS17_PRVS" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Predictive Variance Reduction Search</h4>
            </div>
            <div class="modal-body">
			Predictive Entropy Search (PES) is popular and successful Bayesian optimization
(BO) strategy. It finds a point to maximize the information gained about the
optima of an unknown function. However, PES is computationally expensive and
thus is not scalable to large-scale experiment when the number of observations
and dimensions are large. We propose a new scheme - the Predictive Variance
Reduction Search (PVRS) - to find the best “informative” point which reduces the
predictive variance of the Gaussian process model at the optimum locations. We
draw a connection between our PVRS to the existing PES. Our novel modification
will be beneficial in three ways. First, PVRS directly reduces the uncertainty at
the optimum representative points, like the PES. Second, PVRS can be computed
cheaply in closed-form, unlike the approximations made in PES. Third, the PVRS
is simple and easy to implement. As a result, the proposed PVRS gains huge speed
up for scalable BO whilst showing comparable optimization efficiency.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
<a href="https://bayesopt.github.io/papers/2017/13.pdf" class="label label-info">PDF</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="https://bayesopt.github.io/papers/2017/14.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Distance Exploration for Scalable Batch Bayesian Optimization</strong><br>

         <i>NIPS Workshop on Bayesian Optimization</i>, (<strong>NIPSW</strong>),    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractNIPS17_DE" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractNIPS17_DE" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Distance Exploration for Scalable Batch Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization (BO) is posed as a sequential problem where each experiment
is completed before selecting a next one. However, it is often desirable
to simultaneously explore using batches of parameters, especially when parallel
processing facilities are available. Existing works have addressed batch BO in
different ways. Still, the existing approaches are not scalable to large batch size
or when the function evaluations are cheap. In this paper, we propose a computationally
efficient batch Bayesian optimization based on a new exploration strategy
using geometric distance. Our relaxation reduces the complexity in computing
batch BO and also provides an alternative way for exploration, selecting a point
far from the already observed locations. We theoretically formulate that our new
strategy is a special case of the standard GP variance. We derive convergence
analysis for the proposed batch BO approach. We present extensive experiments to
show that our distance-based approach outperforms the state-of-the-art methods in
both computational efficiency and performance.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
<a href="https://bayesopt.github.io/papers/2017/14.pdf" class="label label-info">PDF</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="https://bayesopt.github.io/papers/2017/16.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    C. Li, S. Gupta, S. Rana, <strong>V. Nguyen</strong>, S. Venkatesh <br>
	    <strong>Bayesian Optimization with Monotonicity Information</strong><br>

        <i>NIPS Workshop on Bayesian Optimization</i>, (<strong>NIPSW</strong>),    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractNIPS17_Monotonicity" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractNIPS17_Monotonicity" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Optimization with Monotonicity Information</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization (BO) has been demonstrated to be an efficient tool to
globally optimize an expensive black-box function. Currently, however only a few
works have explored the use of domain knowledge in BO to gain further efficiency.
In this paper we discuss a particular form of prior information - the monotonicity
of the underlying function with respect to one or more certain variables. Given the
monotonicity information, we first detect the monotonic direction such as increasing
or decreasing at each iteration. We then incorporate the detected monotonic
direction into our proposed BO algorithm. We show the utility of our algorithm
in target value optimization problems. Through the simulation we demonstrate
the correctness of the proposed algorithm in discovering the monotonic direction.
We also demonstrate the superiority of our algorithm in a real-world experimental
optimization for short polymer fiber with the target geometric properties.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
<a href="https://bayesopt.github.io/papers/2017/16.pdf" class="label label-info">PDF</a>
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v77/nguyen17a/nguyen17a.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Regret for Expected Improvement over the Best-Observed Value and Stopping Condition</strong><br>

        <i>Proceedings of The 9th Asian Conference on Machine Learning</i>, (<strong>ACML</strong>),  pp. 279-294,    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractACML17" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractACML17" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Regret for Expected Improvement over the Best-Observed Value and Stopping Condition</h4>
            </div>
            <div class="modal-body">
Bayesian optimization (BO) is a sample-efficient method for global optimization of expensive,
noisy, black-box functions using probabilistic methods. The performance of a BO method depends
on its selection strategy through the acquisition function. Expected improvement (EI) is one of
the most widely used acquisition functions for BO that finds the expectation of the improvement
function over the incumbent. The incumbent is usually selected as the best-observed value so far,
termed as y^max (for the maximizing problem). Recent work has studied the convergence rate for
EI under some mild assumptions or zero noise of observations. Especially, the work of Wang and
de Freitas (2014) has derived the sublinear regret for EI under a stochastic noise. However, due to
the difficulty in stochastic noise setting and to make the convergent proof feasible, they use an alternative
choice for the incumbent as the maximum of the Gaussian process predictive mean, \mu^max.
This modification makes the algorithm computationally inefficient because it requires an additional
global optimization step to estimate mmax that is costly and may be inaccurate. To address this
issue, we derive a sublinear convergence rate for EI using the commonly used ymax. Moreover, our
analysis is the first to study a stopping criteria for EI to prevent unnecessary evaluations. Our analysis
complements the results of Wang and de Freitas (2014) to theoretically cover two incumbent
settings for EI. Finally, we demonstrate empirically that EI using y^max is both more computationally
efficiency and more accurate than EI using \mu^max.
			</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="http://proceedings.mlr.press/v77/nguyen17a/nguyen17a.pdf" class="label label-info">PDF</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/8215507/">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Bayesian Optimization in Weakly Specified Search Space</strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 347-356,    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICDM17_FBO" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICDM17_FBO" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Optimization in Weakly Specified Search Space</h4>
            </div>
            <div class="modal-body">
Bayesian optimization (BO) has recently emerged
as a powerful and flexible tool for hyper-parameter tuning and
more generally for the efficient global optimization of expensive
black-box functions. Systems implementing BO has successfully
solved difficult problems in automatic design choices and machine
learning hyper-parameters tunings. Many recent advances in
the methodologies and theories underlying Bayesian optimization
have extended the framework to new applications and provided
greater insights into the behavior of these algorithms. Still, these
established techniques always require a user-defined space to
perform optimization. This pre-defined space specifies the ranges
of hyper-parameter values. In many situations, however, it can
be difficult to prescribe such spaces, as a prior knowledge is
often unavailable. Setting these regions arbitrarily can lead to
inefficient optimization - if a space is too large, we can miss the
optimum with a limited budget, on the other hand, if a space is
too small, it may not contain the optimum point that we want to
get. The unknown search space problem is intractable to solve in
practice. Therefore, in this paper, we narrow down to consider
specifically the setting of “weakly specified” search space for
Bayesian optimization. By weakly specified space, we mean that
the pre-defined space is placed at a sufficiently good region so that
the optimization can expand and reach to the optimum. However,
this pre-defined space need not include the global optimum.
We tackle this problem by proposing the filtering expansion
strategy for Bayesian optimization. Our approach starts from
the initial region and gradually expands the search space. We
develop an efficient algorithm for this strategy and derive its
regret bound. These theoretical results are complemented by an
extensive set of experiments on benchmark functions and two
real-world applications which demonstrate the benefits of our
proposed approach.</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://github.com/ntienvu/ICDM2017_FBO" class="label label-success">Code</a>
		<a href="" class="label label-danger">Selected as Best Papers, Invited for KAIS</a>
	  <a href="http://ieeexplore.ieee.org/document/8215507/" class="label label-info">PDF</a>


  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/8215498/">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    T. Le, K. Nguyen, <strong>V. Nguyen</strong>, T. D. Nguyen, D. Phung <br>
	    <strong>GoGP: Fast Online Regression with Gaussian Processes</strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 257-266,    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICDM17_GoGP" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICDM17_GoGP" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">GoGP: Fast Online Regression with Gaussian Processes </h4>
            </div>
            <div class="modal-body">
One of the most current challenging problems in Gaussian process regression (GPR) is to handle large-scale datasets and to accommodate an online learning setting where data arrive 
irregularly on the fly. In this paper, we introduce a novel online Gaussian process model that could scale with massive datasets. Our approach is formulated based on alternative 
representation of the Gaussian process under geometric and optimization views, hence termed geometric-based online GP (GoGP). We developed theory to guarantee that with a good convergence 
rate our proposed algorithm always produces a (sparse) solution which is close to the true optima to any arbitrary level of approximation accuracy specified a priori. 
Furthermore, our method is proven to scale seamlessly not only with large-scale datasets, but also to adapt accurately with streaming data. 
We extensively evaluated our proposed model against state-of-the-art baselines using several large-scale datasets for online regression task. 
The experimental results show that our GoGP delivered comparable, or slightly better, predictive performance while achieving a magnitude of computational speedup compared 
with its rivals under online setting. 
More importantly, its convergence behavior is guaranteed through our theoretical analysis, which is rapid and stable while achieving lower errors.
			</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://github.com/ntienvu/GoGP" class="label label-success">Code</a>
	  <a href="" class="label label-danger">Selected as Best Papers, Invited for KAIS</a>
	  <a href="http://ieeexplore.ieee.org/document/8215498/" class="label label-info">PDF</a>

  </div>
</div>

<div class="media">
  <a class="pull-left thumbnail" href="http://www.jmlr.org/papers/v18/16-191.html">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    T. Le, T. D Nguyen, <strong>V. Nguyen</strong>, D. Phung, <br>
	    <strong>Approximation Vector Machines for Large-scale Online Learning</strong><br>

                   <i>Journal of Machine Learning Research</i>, (<strong>JMLR</strong>), 2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractJMLR17" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractJMLR17" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Approximation Vector Machines for Large-scale Online Learning</h4>
            </div>
            <div class="modal-body">
			One of the most challenging problems in kernel online learning is to bound the model size
and to promote model sparsity. Sparse models not only improve computation and memory
usage, but also enhance the generalization capacity – a principle that concurs with the law
of parsimony. However, inappropriate sparsity modeling may also significantly degrade the
performance. In this paper, we propose Approximation Vector Machine (AVM), a model
that can simultaneously encourage sparsity and safeguard its risk in compromising the performance.
In an online setting context, when an incoming instance arrives, we approximate
this instance by one of its neighbors whose distance to it is less than a predefined threshold.
Our key intuition is that since the newly seen instance is expressed by its nearby neighbor
the optimal performance can be analytically formulated and maintained. We develop
theoretical foundations to support this intuition and further establish an analysis for the
common loss functions including Hinge, smooth Hinge, and Logistic (i.e., for the classifi-
cation task) and `1, `2, and ε-insensitive (i.e., for the regression task) to characterize the
gap between the approximation and optimal solutions. This gap crucially depends on two
key factors including the frequency of approximation (i.e., how frequent the approximation
operation takes place) and the predefined threshold. We conducted extensive experiments
for classification and regression tasks in batch and online modes using several benchmark
datasets. The quantitative results show that our proposed AVM obtained comparable predictive
performances with current state-of-the-art methods while simultaneously achieving
significant computational speed-up due to the ability of the proposed AVM in maintaining
the model size.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
		  <a href="http://www.jmlr.org/papers/volume18/16-191/16-191.pdf" class="label label-info">PDF</a>
			  <a href="https://github.com/ntienvu/avm" class="label label-success">Code</a>


  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v70/rana17a/rana17a.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    S. Rana, C. Li, S. Gupta, <strong>V. Nguyen</strong>, S. Venkatesh <br>
	    <strong>High Dimensional Bayesian Optimization with Elastic Gaussian Process</strong><br>

         <i>Proceedings of the 34th International Conference on Machine Learning</i>, (<strong>ICML</strong>), pp 2883-2891,   2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICML17" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICML17" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">High Dimensional Bayesian Optimization with Elastic Gaussian Process</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization is an efficient way to optimize expensive black-box functions such as designing a new product with highest quality or hyperparameter tuning of a machine learning algorithm. However, it has a serious limitation when the parameter space is high-dimensional as Bayesian optimization crucially depends on solving a global optimization of a surrogate utility function in the same sized dimensions. The surrogate utility function, known commonly as acquisition function is a continuous function but can be extremely sharp at high dimension - having only a few peaks marooned in a large terrain of almost flat surface. Global optimization algorithms such as DIRECT are infeasible at higher dimensions and gradient-dependent methods cannot move if initialized in the flat terrain. We propose an algorithm that enables local gradient-dependent algorithms to move through the flat terrain by using a sequence of gross-to-finer Gaussian process priors on the objective function as we leverage two underlying facts - a) there exists a large enough length-scales for which the acquisition function can be made to have a significant gradient at any location in the parameter space, and b) the extrema of the consecutive acquisition functions are close although they are different only due to a small difference in the length-scales. Theoretical guarantees are provided and experiments clearly demonstrate the utility of the proposed method at high dimension using both benchmark test functions and real-world case studies.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	  	          <a href="http://proceedings.mlr.press/v70/rana17a/rana17a.pdf" class="label label-info">PDF</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://www.programmaster.org/PM/PM.nsf/ApprovedAbstracts/DE70A52EEC78DB2F85258049005C4776?OpenDocument">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    P. Sanders, S. Rana, J. Licavoli, S. Gupta, <strong>V. Nguyen</strong>, S. Venkatesh <br>
	    <strong>Bayesian Optimization of Superalloy Design</strong><br>
         <i>4th World Congress on Integrated Computational Materials Engineering</i> 2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICME17" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICME17" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Optimization of Superalloy Design</h4>
            </div>
            <div class="modal-body">
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
		<a href="" class="label label-danger">Best Poster Award</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://static.ijcai.org/proceedings-2017/0355.pdf">
    <img src="./img/paper/ijcai2017.png" alt="">
  </a>
  <div class="media-body">
     <strong>V. Nguyen</strong>, D. Phung, T. Le, S. Venkatesh, H. Bui <br>
	    <strong>Discriminative Bayesian Nonparametric Clustering</strong><br>

         <i>Proceedings of International Joint Conference on Artificial Intelligence</i>, (<strong>IJCAI</strong>), pp 2550-2556,    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractIJCAI_Vu" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractIJCAI_Vu" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Discriminative Bayesian Nonparametric Clustering</h4>
            </div>
            <div class="modal-body">
			We propose a general framework for discriminative
Bayesian nonparametric clustering to promote
the inter-discrimination among the learned clusters
in a fully Bayesian nonparametric (BNP) manner.
Our method combines existing BNP clustering and
discriminative models by enforcing latent cluster
indices to be consistent with the predicted labels
resulted from probabilistic discriminative model.
This formulation results in a well-defined generative
process wherein we can use either logistic regression
or SVM for discrimination. Using the proposed
framework, we develop two novel discriminative
BNP variants: the discriminative Dirichlet
process mixtures, and the discriminative-state in-
finite HMMs for sequential data. We develop ef-
ficient data-augmentation Gibbs samplers for posterior
inference. Extensive experiments in image
clustering and dynamic location clustering demonstrate
that by encouraging discrimination between
induced clusters, our model enhances the quality of
clustering in comparison with the traditional generative
BNP models.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	  	          <a href="http://static.ijcai.org/proceedings-2017/0355.pdf" class="label label-info">PDF</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://static.ijcai.org/proceedings-2017/0291.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    C. Li, S. Gupta, S. Rana, <strong>V. Nguyen</strong>, S. Venkatesh, A. Shilton <br>
	    <strong>High Dimensional Bayesian Optimization Using Dropout</strong><br>

         <i>Proceedings of International Joint Conference on Artificial Intelligence  </i>, (<strong>IJCAI</strong>), pp 2096-2102,   2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractIJCAI_Cheng" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractIJCAI_Cheng" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">High Dimensional Bayesian Optimization Using Dropout</h4>
            </div>
            <div class="modal-body">
			Scaling Bayesian optimization to high dimensions
is challenging task as the global optimization of
high-dimensional acquisition function can be expensive
and often infeasible. Existing methods depend
either on limited “active” variables or the additive
form of the objective function. We propose
a new method for high-dimensional Bayesian optimization,
that uses a dropout strategy to optimize
only a subset of variables at each iteration. We derive
theoretical bounds for the regret and show how
it can inform the derivation of our algorithm. We
demonstrate the efficacy of our algorithms for optimization
on two benchmark functions and two realworld
applications - training cascade classifiers and
optimizing alloy composition
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	  	          <a href="http://static.ijcai.org/proceedings-2017/0291.pdf" class="label label-info">PDF</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="https://bayesopt.github.io/papers/2016/Nguyen.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. K. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Think Globally, Act Locally: a Local Strategy for Bayesian Optimization</strong><br>

         <i>NIPS Workshop on Bayesian Optimization</i>, (<strong>NIPSW</strong>),    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractNIPS16_ELI" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractNIPS16_ELI" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Think Globally, Act Locally: a Local Strategy for Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization (BO) is a sample-efficient method for improving the performance of machine learning algorithms and laboratory experiments. We exploit the local property in BO to develop a new acquisition function, the expected local
improvement (ELI) as an alternative to Expected Improvement (EI), aiming to address two underlying issues. First, we reduce the flatland issue in high dimension and second we allow greater explorative choices for batch BO unlike the existing strategies. We derive the convergence analysis using simple regret bound. We further demonstrate that the proposed strategy gains substantial performance improvement over the state-of-the-art baselines using the benchmark functions and
real experiments on sequential and batch BO.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	  	          <a href="https://bayesopt.github.io/papers/2016/Nguyen.pdf" class="label label-info">PDF</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="https://bayesopt.github.io/papers/2016/Li.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    C. Li, S. K. Gupta, S. Rana, <strong>V. Nguyen</strong>, S. Venkatesh <br>
	    <strong>High Dimensional Bayesian Optimization with Elastic Gaussian Process</strong><br>

         <i>NIPS Workshop on Bayesian Optimization</i>, (<strong>NIPSW</strong>),    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractNIPS16_EGP" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractNIPS16_EGP" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Think Globally, Act Locally: a Local Strategy for Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization depends on solving a global optimization of a acquisition function. However, the acquisition function can be extremely sharp at high dimension
- having only a few peaks marooned in a large terrain of almost flat surface. Global optimization algorithms such as DIRECT are infeasible at higher dimensions
and gradient-dependent methods cannot move if initialized in the flat terrain. We propose an algorithm that enables local gradient-dependent algorithms to move
through the flat terrain by using a sequence of gross-to-finer Gaussian process priors on the objective function. Experiments clearly demonstrate the utility of the
proposed method at high dimension using synthetic and real-world case studies.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	  	          <a href="https://bayesopt.github.io/papers/2016/Li.pdf" class="label label-info">PDF</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v63/nguyen93.html">
    <img src="./img/paper/acml2016.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. K. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>A Bayesian Nonparametric Approach for Multi-label Classification</strong><br>

         <i>Proceedings of The 8th Asian Conference on Machine Learning</i>, (<strong>ACML</strong>), pp 254-269,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractACML_BNMC" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractACML_BNMC" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">A Bayesian Nonparametric Approach for Multi-label Classification</h4>
            </div>
            <div class="modal-body">
			Many real-world applications require multi-label classification where multiple target labels
are assigned to each instance. In multi-label classification, there exist the intrinsic correlations
between the labels and features. These correlations are beneficial for multi-label
classification task since they reflect the coexistence of the input and output spaces that can
be exploited for prediction. Traditional classification methods have attempted to reveal
these correlations in different ways. However, existing methods demand expensive computation
complexity for finding such correlation structures. Furthermore, these approaches
can not identify the suitable number of label-feature correlation patterns. In this paper, we
propose a Bayesian nonparametric (BNP) framework for multi-label classification that can
automatically learn and exploit the unknown number of multi-label correlation. We utilize
the recent techniques in stochastic inference to derive the cheap (but efficient) posterior
inference algorithm for the model. In addition, our model can naturally exploit the useful
information from missing label samples. Furthermore, we extend the model to update parameters
in an online fashion that highlights the flexibility of our model against the existing
approaches. We compare our method with the state-of-the-art multi-label classification algorithms
on real-world datasets using both complete and missing label settings. Our model
achieves better classification accuracy while our running time is consistently much faster
than the baselines in an order of magnitude.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	          <a href="http://proceedings.mlr.press/v63/nguyen93.html" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ACML2016_BNMC" class="label label-success">Code</a>
		<a href="https://www.youtube.com/watch?v=-EE-I2IpQbo" class="label label-success">Youtube Demo</a>
		<a href="" class="label label-danger">Best Paper Runner Up Award</a>
		<a href="" class="label label-danger">Best Poster Award</a>

	  
  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v63/nguyen19.html">
    <img src="./img/paper/acml2016.png" alt="">
  </a>
  <div class="media-body">
    K. Nguyen, T. Le, <strong>V. Nguyen</strong>, T. D. Nguyen, D. Phung <br>
	    <strong>Multiple Kernel Learning with Data Augmentation</strong><br>

         <i>Proceedings of The 8th Asian Conference on Machine Learning</i>, (<strong>ACML</strong>),  pp 49-64,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractACML_MKL" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractACML_MKL" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Multiple Kernel Learning with Data Augmentation</h4>
            </div>
            <div class="modal-body">
			The motivations of multiple kernel learning (MKL) approach are to increase kernel expressiveness
capacity and to avoid the expensive grid search over a wide spectrum of kernels.
A large amount of work has been proposed to improve the MKL in terms of the computational
cost and the sparsity of the solution. However, these studies still either require an
expensive grid search on the model parameters or scale unsatisfactorily with the numbers
of kernels and training samples. In this paper, we address these issues by conjoining MKL,
Stochastic Gradient Descent (SGD) framework, and data augmentation technique. The
pathway of our proposed method is developed as follows. We first develop a maximum-aposteriori
(MAP) view for MKL under a probabilistic setting and described in a graphical
model. This view allows us to develop data augmentation technique to make the inference
for finding the optimal parameters feasible, as opposed to traditional approach of training
MKL via convex optimization techniques. As a result, we can use the standard SGD
framework to learn weight matrix and extend the model to support online learning. We
validate our method on several benchmark datasets in both batch and online settings. The
experimental results show that our proposed method can learn the parameters in a principled
way to eliminate the expensive grid search while gaining a significant computational
speedup comparing with the state-of-the-art baselines.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	          <a href="http://proceedings.mlr.press/v63/nguyen19.html" class="label label-info">PDF</a>



	  
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/7837957/">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Rana, S. K. Gupta, C. Li, S. Venkatesh <br>
	    <strong>Budgeted Batch Bayesian Optimization</strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 1107-1112,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractB3O_ICDM" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractB3O_ICDM" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Budgeted Batch Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			Parameter settings profoundly impact the performance
of machine learning algorithms and laboratory experiments.
The classical trial-error methods are exponentially expensive
in large parameter spaces, and Bayesian optimization (BO)
offers an elegant alternative for global optimization of black box
functions. In situations where the functions can be evaluated
at multiple points simultaneously, batch Bayesian optimization
is used. Current batch BO approaches are restrictive in fixing
the number of evaluations per batch, and this can be wasteful
when the number of specified evaluations is larger than the
number of real maxima in the underlying acquisition function.
We present the budgeted batch Bayesian optimization (B3O) for
hyper-parameter tuning and experimental design - we identify
the appropriate batch size for each iteration in an elegant
way. In particular, we use the infinite Gaussian mixture model
(IGMM) for automatically identifying the number of peaks in
the underlying acquisition functions. We solve the intractability
of estimating the IGMM directly from the acquisition function
by formulating the batch generalized slice sampling to efficiently
draw samples from the acquisition function. We perform extensive
experiments for benchmark functions and two real world
applications - machine learning hyper-parameter tuning and
experimental design for alloy hardening. We show empirically
that the proposed B3O outperforms the existing fixed batch BO
approaches in finding the optimum whilst requiring a fewer
number of evaluations, thus saving cost and time.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	          <a href="http://ieeexplore.ieee.org/document/7837957/" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ICDM2016_B3O" class="label label-success">Code</a>
	  
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/7837958/">
    <img src="./img/paper/ICDM2016_OLR.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, T. D. Nguyen, T. Le, S. Venkatesh, D. Phung.  <br>
	    <strong>One-pass Logistic Regression for Label-drift and Large-scale Classification on Distributed Systems </strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 1113-1118,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractOLR_ICDM" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractOLR_ICDM" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">One-pass Logistic Regression for Label-drift and Large-scale Classification on Distributed Systems</h4>
            </div>
            <div class="modal-body">
			Logistic regression (LR) is at the cornerstone of classification. Its extension for multiclass classification is the
workhorse in industry, where a set of predefined classes is required. The model, however, fails to work in the case where
the class labels are not known in advance, a problem we term label-drift classification, in a similar spirit of the so-called conceptdrift
problem in the literature. Label-drift classification problem naturally occurs in many applications, especially in the context
of streaming and online settings where the incoming data may contain samples categorized with new classes that have not
been previously seen. Additionally, in the wave of big data, traditional LR methods may fail due to their expense of running
time and label-drift requirements. In this paper, we introduce a novel variant of LR, namely one-pass logistic regression (OLR)
to offer a principled treatment for large-scale and label-drift classifications. Our key contribution is the derivation of sufficient
statistic update for MAP estimation of Polya-Gamma augmentation for LR. Manipulating these sufficient statistics is convenient,
allowing our proposed method to efficiently perform the labeldrift classification under an online setting without retraining the
model from scratch. To handle large-scale classification for big data, we further extend our OLR to a distributed setting for
parallelization, termed sparkling OLR (Spark-OLR). We demonstrate the scalability of our proposed methods on large-scale
datasets with more than one hundred million data points. The experimental results show that the predictive performances of our
methods are comparable or better than those of state-of-the-art baselines whilst the execution time is much faster at an order of
magnitude. To measure the inherent trade-off between speed and accuracy, we propose quadrant visualization and quadrant score,
on which our proposed model outperforms other methods on all datasets. In addition, the OLR and Spark-OLR are invariant
to data shuffling and have no hyperparameter to tune that significantly benefits data practitioners and overcomes the curse
of big data cross-validation to select optimal hyperparameters.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	          <a href="http://ieeexplore.ieee.org/document/7837958/" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ICDM2016_OLR" class="label label-success">Code</a>
  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="https://link.springer.com/chapter/10.1007/978-3-319-50127-7_22">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    T. D. Nguyen, S. K. Gupta, S. Rana, <strong>V. Nguyen</strong>, S. Venkatesh, K. Deane, P. Sanders<br>
	    <strong>Cascade Bayesian Optimization</strong><br>

         <i>Proceedings The 29th Australasian Joint Conference on Artificial Intelligence</i>, (<strong>AI</strong>),  pp 268-280,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractCascadeBO" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractCascadeBO" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Cascade Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			Multi-stage cascade processes are fairly common, especially in manufacturing
industry. Precursors or raw materials are transformed at each stage before
being used as the input to the next stage. Setting the right control parameters
at each stage is important to achieve high quality products at low cost. Finding
the right parameters via trial and error approach can be time consuming. Bayesian
optimization is an efficient way to optimize costly black-box function. We extend
the standard Bayesian optimization approach to the cascade process through formulating
a series of optimization problems that are solved sequentially from the
final stage to the first stage. Epistemic uncertainties are effectively utilized in the
formulation. Further, cost of the parameters are also included to find cost-efficient
solutions. Experiments performed on a simulated testbed of Al-Sc heat treatment
through a three-stage process showed considerable efficiency gain over a naïve
optimization approach.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	 	  	  	          <a href="https://link.springer.com/chapter/10.1007/978-3-319-50127-7_22" class="label label-info">PDF</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="https://papers.nips.cc/paper/6560-dual-space-gradient-descent-for-online-learning.pdf">
    <img src="./img/paper/NIPS2016.png" alt="">
  </a>
  <div class="media-body">
    T. Le, T.D. Nguyen,<strong>V. Nguyen</strong>, D. Phung.  <br>
	    <strong>Dual Space Gradient Descent for Online Learning</strong><br>

         <i>Advances in Neural Information Processing Systems</i>, (<strong>NIPS</strong>),  pp 4583-4591,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractNIPS" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractNIPS" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Dual Space Gradient Descent for Online Learning</h4>
            </div>
            <div class="modal-body">
			One crucial goal in kernel online learning is to bound the model size. Common
approaches employ budget maintenance procedures to restrict the model sizes using
removal, projection, or merging strategies. Although projection and merging, in the
literature, are known to be the most effective strategies, they demand extensive com
putation whilst removal strategy fails to retain information of the removed vectors.
An alternative way to address the model size problem is to apply random features
to approximate the kernel function. This allows the model to be maintained directly
in the random feature space, hence effectively resolve the curse of kernelization.
However, this approach still suffers from a serious shortcoming as it needs to use a
high dimensional random feature space to achieve a sufficiently accurate kernel
 approximation. Consequently, it leads to a significant increase in the computational
 cost. To address all of these aforementioned challenges, we present in this paper
 the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes
 random features as an auxiliary space to maintain information from data points
 removed during budget maintenance. Consequently, our approach permits the
budget to be maintained in a simple, direct and elegant way while simultaneously
 mitigating the impact of the dimensionality issue on learning performance. We
 further provide convergence analysis and extensively conduct experiments on five
 real-world datasets to demonstrate the predictive performance and scalability of
 our proposed method in comparison with the state-of-the-art baselines.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
	          <a href="https://papers.nips.cc/paper/6560-dual-space-gradient-descent-for-online-learning.pdf" class="label label-info">PDF</a>

			  
	    </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://www.sciencedirect.com/science/article/pii/S1574119216302097">
    <img src="./img/paper/PERCOM2016.jpg" alt="">
  </a>
  <div class="media-body">
    T. Nguyen, <strong>V. Nguyen</strong>, F.D. Salim, D. V Le, D. Phung.  <br>
	    <strong>A Simultaneous Extraction of Context and Community from pervasive signals using nested Dirichlet process</strong><br>

        <i>Pervasive and Mobile Computing</i>, Elsevier,  2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractPMC16" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractPMC16" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">A Simultaneous Extraction of Context and Community from pervasive signals using nested Dirichlet process</h4>
            </div>
            <div class="modal-body">
Understanding user contexts and group structures plays a central role in pervasive computing. These contexts and community structures are complex to mine from data collected in the wild due to the unprecedented growth of data, noise, uncertainties and complexities. Typical existing approaches would first extract the latent patterns to explain the human dynamics or behaviors and then use them as a way to consistently formulate numerical representations for community detection, often via a clustering method. While being able to capture high-order and complex representations, these two steps are performed separately. More importantly, they face a fundamental difficulty in determining the correct number of latent patterns and communities. This paper presents an approach that seamlessly addresses these challenges to simultaneously discover latent patterns and communities in a unified Bayesian nonparametric framework. Our Simultaneous Extraction of Context and Community (SECC) model roots in the nested Dirichlet process theory which allows nested structure to be built to summarize data at multiple levels. We demonstrate our framework on five datasets where the advantages of the proposed approach are validated.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://www.sciencedirect.com/science/article/pii/S1574119216302097" class="label label-info">PDF</a>
  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900120">
    <img src="./img/paper/ICPR2016.PNG" alt="">
  </a>
  <div class="media-body">
    C. Li, S. K Gupta, S. Rana, <strong>V. Nguyen</strong>, S. Venkatesh, D. Ashley, T. Livingston   <br>
	    <strong>Multiple Adverse Effects Prediction in Longitudinal Cancer Treatment</strong><br>

         <i>Proceedings of the 23rd International Conference on Pattern Recognition</i>, (ICPR),  pp.  3156-3161,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractLongitudinalCancerICPR" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractLongitudinalCancerICPR" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Multiple Adverse Effects Prediction in Longitudinal Cancer Treatment</h4>
            </div>
            <div class="modal-body">
			Adverse effects, such as voice change and fatigue, are prevalent in cancer treatment duration. These adverse effects have been significant burden for patients physically and emotionally. Predicting multiple adverse effects becomes important for patients and oncologists. In this paper, we formulate the prediction of multiple adverse effects in cancer treatment as a longitudinal multiple-output regression problem. The
correlated multiple outputs are first decoupled to uncorrelated ones in a new output space. We then propose a comprehensive framework to capture the empirical loss between the predicted value and the ground truth in the transformed space and the temporal smoothness at neighboring prediction points. Experiments were performed on one synthetic data and two realworld datasets including radiotherapy and chemotherapy treatments. Results in terms of root mean square errors (RMSE) and R-value show that our proposed approach is promising for the longitudinal multipleoutput regression problem
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	          <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900120" class="label label-info">PDF</a>

	  		<a href="" class="label label-danger">Finalist Best Intel Scientific Paper Award - Track 5</a>

	  
  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/7900198/">
    <img src="./img/paper/ICPR2016.PNG" alt="">
  </a>
  <div class="media-body">
    T-B. Nguyen, <strong>V. Nguyen</strong>, S. Venkatesh, D. Phung.  <br>
	    <strong>MCNC: Multi-channel Nonparametric Clustering from Heterogeneous Data </strong><br>
         <i>Proceedings of the 23rd International Conference on Pattern Recognition</i>, (ICPR), pp 3633-3638,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractMCNC" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractMCNC" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">MCNC: Multi-channel Nonparametric Clustering from Heterogeneous Data </h4>
            </div>
            <div class="modal-body">
			Bayesian nonparametric (BNP) models have recently become popular due to their flexibility in identifying the unknown number of clusters. However, they have difficulties handling
heterogeneous data from multiple sources. Existing BNP methods either treat each of these sources independently – hence do not get benefits from the correlating information between them, or
require to explicitly specify data sources as primary and context channels. In this paper, we present a BNP framework, termed MCNC, which has the ability to (1) discover co-patterns from
multiple sources; (2) explore multi-channel data simultaneously and treat them equally; (3) automatically identify a suitable number of patterns from data; and (4) handle missing data. The
key idea is to utilize a richer base measure of a BNP model being a product-space. We demonstrate our framework on synthetic and real-world datasets to discover the identity–location–time (a.k.a
who–where–when) patterns. The experimental results highlight the effectiveness of our MCNC framework in both cases of complete and missing data.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	          <a href="http://ieeexplore.ieee.org/document/7900198/" class="label label-info">PDF</a>

	  	  	  <a href="https://github.com/ntienvu/InteractiveDemo_MCNC_ICPR2016" class="label label-success">Visualization</a>
	  		<a href="" class="label label-danger">Finalist Best IBM Student Paper Award - Track 1</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/7899683/">
    <img src="./img/paper/ICPR2016.PNG" alt="">
  </a>
  <div class="media-body">
    T Nguyen, <strong>V Nguyen</strong>, T Le, D Phung.  <br>
	    <strong>Distributed Data Augmented Support Vector Machine on Spark </strong><br>

         <i>Proceedings of the 23rd International Conference on Pattern Recognition</i>, (ICPR),  pp 498-503,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractSparkSVM_ICPR" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractSparkSVM_ICPR" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Distributed Data Augmented Support Vector Machine on Spark  </h4>
            </div>
            <div class="modal-body">
			Support vector machines (SVMs) are widely-used for classification in machine learning and data mining tasks. However, they traditionally have been applied to small to medium
datasets. Recent need to scale up with data size has attracted research attention to develop new methods and implementation for SVM to perform tasks at scale. Distributed SVMs
are relatively new and studied recently, but the distributed implementation for SVM with data augmentation has not been developed. This paper introduces a distributed data augmentation
implementation for SVM on Apache Spark, a recent advanced and popular platform for distributed computing that has been employed widely in research as well as in industry. We term our
implementation sparkling vector machine (SkVM) which supports both classification and regression tasks by scanning through the data exactly once. In addition, we further develop a framework
to handle the data with new classes arriving under an online classification setting where new data points can have labels that have not previously seen – a problem we term label-drift
classification. We demonstrate the scalability of our proposed method on large-scale datasets with more than one hundred million data points. The experimental results show that the
predictive performances of our method are comparable or better than those of baselines whilst the execution time is much faster at an order of magnitude.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	          <a href="http://ieeexplore.ieee.org/document/7899683/" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/skvm" class="label label-success">Code</a>
	  
  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/7796925/">
    <img src="./img/paper/rsz_dsaa16.png" alt="">
  </a>
  <div class="media-body">
    T-B Nguyen, <strong>V Nguyen</strong>, T Nguyen, M Kumar, S Venkatesh, D Phung.  <br>
	    <strong>Learning Multifaceted Latent Activities from Heterogeneous Mobile Data </strong><br>

         <i>Proceedings of the 3rd International Conference on Data Science and Advanced Analytics</i>, (DSAA),  pp 389-398,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractDSAA16" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractDSAA16" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Learning Multifaceted Latent Activities from Heterogeneous Mobile Data</h4>
            </div>
            <div class="modal-body">
			Inferring abstract contexts and activities from heterogeneous data is vital to context-aware ubiquitous applications but still remains one of the most challenging problems. Recent advances in Bayesian nonparametric machine learning, in particular the theory of topic models based on Hierarchical Dirichlet Process (HDP), has provided an elegant solution towards these challenges. However, limited existing methods have addressed the problem of inferring latent multifaceted activities and contexts from heterogeneous data sources such as those collected from mobile devices. In this paper, we extend the original HDP to model heterogeneous data using a richer structure of the base measure being a product-space. The proposed model, called product-space HDP (PS-HDP), naturally handles the heterogeneous data from multiple sources and identify the unknown number of latent structures in a principle way. Although this framework is generic, our current work primarily focuses on inferring (latent) threefold activities of who-when-where simultaneously, which corresponds to inducing activities from data collected for identity, location and
time. We demonstrate our model on synthetic data as well as on the StudentLife dataset. We provide analysis on the discovered activities and patterns to demonstrate the merit of the model. We also quantitatively evaluate the performance of PS-HDP model using standard metrics including F1-score, NMI, RI, purity, and compare them with well-known existing baseline methods.		
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	          <a href="http://ieeexplore.ieee.org/document/7796925/" class="label label-info">PDF</a>

	  	  <a href="https://github.com/ntienvu/InteractiveDemo_PSHDP_DSAA16" class="label label-success">Visualization</a>
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://auai.org/uai2016/proceedings/papers/110.pdf">
    <img src="./img/paper/AISTATS2016.jpg" alt="">
  </a>
  <div class="media-body">
    T Le, P Duong, M Dinh, T Nguyen, <strong>V Nguyen</strong>, D Phung.  <br>
	    <strong>Budgeted Semi-supervised Support Vector Machine</strong><br>

         <i>Proceedings of the 32nd Conference on Uncertainty in Artificial Intelligence</i>, (<strong>UAI</strong>),  pp 377--386,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractUAI2016" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractUAI2016" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Budgeted Semi-supervised Support Vector Machine </h4>
            </div>
            <div class="modal-body">
Due to the prevalence of unlabeled data, semisupervised learning has drawn significant attention and has been found applicable in many realworld applications. In this paper, we present the so-called Budgeted Semi-supervised Support Vector Machine (BS3VM), a method that leverages the excellent generalization capacity of kernel-based method with the adjacent and distributive information carried in a spectral graph for semi-supervised learning purpose. The fact
that the optimization problem of BS3VM can be solved directly in the primal form makes it fast and efficient in memory usage. We validate the proposed method on several benchmark datasets to demonstrate its accuracy and efficiency. The experimental results show that BS3VM can scale up efficiently to the large-scale datasets where it yields a comparable classification accuracy while simultaneously achieving a significant computational speed-up compared with the baselines.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://auai.org/uai2016/proceedings/papers/110.pdf" class="label label-info">PDF</a>
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://jmlr.org/proceedings/papers/v51/le16.pdf">
    <img src="./img/paper/AISTATS2016.jpg" alt="">
  </a>
  <div class="media-body">
    T Le, <strong>V Nguyen</strong>, TD Nguyen, D Phung.  <br>
	    <strong>Nonparametric Budgeted Stochastic Gradient Descent</strong><br>
         <i>Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</i>, (<strong>AISTATS</strong>),  pp 654-572,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractabstractNonparametricBudgetedStochasticGradientDescent" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractabstractNonparametricBudgetedStochasticGradientDescent" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Nonparametric Budgeted Stochastic Gradient Descent</h4>
            </div>
            <div class="modal-body">
One of the most challenging problems in kernel online learning is to bound the model size. 
Budgeted kernel online learning addresses this issue by bounding the model size to a predefined budget. 
However, determining an appropriate value for such predefined budget is arduous. 
In this paper, we propose the Nonparametric Budgeted Stochastic Gradient Descent that allows the model size to automatically grow with data in a principled way. 
We provide theoretical analysis to show that our framework is guaranteed to converge for a large collection of loss functions 
(e.g. Hinge, Logistic, L2, L1, and ε-insensitive) which enables the proposed algorithm to perform both classification and regression tasks without hurting the ideal convergence rate O(1/T) 
of the standard Stochastic Gradient Descent. We validate our algorithm on the real-world datasets to consolidate the theoretical claims.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://jmlr.org/proceedings/papers/v51/le16.pdf" class="label label-info">PDF</a>
      <a href="https://github.com/ntienvu/NonparametricBudgetedSGD" class="label label-success">Code</a>
  </div>
</div>

<div class="media">

  <a class="pull-left thumbnail" href="http://link.springer.com/chapter/10.1007%2F978-3-319-42996-0_11">
    <img src="./img/paper/MLSDA2016.jpg" alt="">
  </a>
  <div class="media-body">
    Thanh-Binh Nguyen, <strong>Vu Nguyen</strong>, Svetha Venkatesh and Dinh Phung.  <br>
	    <strong>Learning Multi-faceted Activities from Heterogeneous Data with the Product Space Hierarchical Dirichlet Processes</strong><br>
         <i>Pacific-Asia Conference on Knowledge Discovery and Data Mining. Proceedings of the 3rd Workshop on Machine Learning for Sensory Data Analysis</i>, (MLSDA),  pp 128-140, 2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractLearningMulti-facetedActivitiesPSHDP" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractLearningMulti-facetedActivitiesPSHDP" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Learning Multi-faceted Activities from Heterogeneous Data with the Product Space Hierarchical Dirichlet Processes</h4>
            </div>
            <div class="modal-body">
Hierarchical Dirichlet processes (HDP) was originally designed and experimented for a single data channel. In this paper we
enhanced its ability to model heterogeneous data using a richer structure for the base measure being a product-space. The enhanced model,
called Product Space HDP (PS-HDP), can (1) simultaneously model
heterogeneous data from multiple sources in a Bayesian nonparametric
framework and (2) discover multilevel latent structures from data to result in different types of topics/latent structures that can be explained
jointly. We experimented with the MDC dataset, a large and real-world
data collected from mobile phones. Our goal was to discover identity–
location–time (a.k.a who-where-when) patterns at different levels (globally for all groups and locally for each group). We provided analysis
on the activities and patterns learned from our model, visualized, compared and contrasted with the ground-truth to demonstrate the merit
of the proposed framework. We further quantitatively evaluated and reported its performance using standard metrics including F1-score, NMI,
RI, and purity. We also compared the performance of the PS-HDP model
with those of popular existing clustering methods (including K-Means,
NNMF, GMM, DP-Means, and AP). Lastly, we demonstrate the ability
of the model in learning activities with missing data, a common problem
encountered in pervasive and ubiquitous computing applications.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://link.springer.com/chapter/10.1007%2F978-3-319-42996-0_11" class="label label-info">PDF</a>
  </div>
</div>

<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7456501&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7456501">
    <img src="./img/paper/PERCOM2016.jpg" alt="">
  </a>
  <div class="media-body">
    T Nguyen, <strong>V Nguyen</strong>, FD Salim, D Phung.  <br>
	    <strong>SECC: Simultaneous Extraction of Context and Community from Pervasive Signals</strong><br>
         <i>Proceedings of 2016 IEEE International Conference on Pervasive Computing and Communications</i>, (<strong>PERCOM</strong>), pp 1-9,  2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractSECC" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractSECC" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">SECC: Simultaneous Extraction of Context and Community from Pervasive Signals</h4>
            </div>
            <div class="modal-body">
Understanding user contexts and group structures plays a central role in pervasive computing. These contexts and community structures are complex to mine from data collected in the 
wild due to the unprecedented growth of data, noise, uncertainties and complexities. Typical existing approaches would first extract the latent patterns to explain the human dynamics 
or behaviors and then use them as the way to consistently formulate numerical representations for community detection, often via a clustering method. While being able to capture highorder 
and complex representations, these two steps are performed separately. More importantly, they face a fundamental difficulty in determining the correct number of latent patterns and communities. 
This paper presents an approach that seamlessly addresses these challenges to simultaneously discover latent patterns and communities in a unified Bayesian nonparametric framework. 
Our Simultaneous Extraction of Context and Community (SECC) model roots in the nested Dirichlet process theory which allows nested structure to be built to explain data at multiple levels. 
We demonstrate our framework on three public datasets where the advantages of the proposed approach are validated.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7456501&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7456501" class="label label-info">PDF</a>
  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://link.springer.com/chapter/10.1007%2F978-3-319-31753-3_3">
    <img src="./img/paper/PAKDD2016.jpg" alt="">
  </a>
  <div class="media-body">
   K. Nguyen, T. Le, <strong>V. Nguyen</strong>, D. Phung  <br>
       <strong>Sparse Adaptive Multi-Hyperplane Machine.</strong><br>
         <i>Advances in Knowledge Discovery and Data Mining</i>, (PAKDD), pp 27-39,  2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractSparseAdaptive" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractSparseAdaptive" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Sparse Adaptive Multi-Hyperplane Machine</h4>
            </div>
            <div class="modal-body">
The Adaptive Multiple-hyperplane Machine (AMM) was recently proposed to deal with large-scale datasets. However, it has no principle to tune the complexity and sparsity levels of the solution.
 Addressing the sparsity is important to improve learning generalization, prediction accuracy and computational speedup. In this paper, we employ the max-margin principle and sparse approach
 to propose a new Sparse AMM (SAMM). We solve the new optimization objective function with stochastic gradient descent (SGD). Besides inheriting the good features of SGD-based learning method 
 and the original AMM, our proposed Sparse AMM provides machinery and flexibility to tune the complexity and sparsity of the solution, making it possible to avoid overfitting and underfitting. 
 We validate our approach on several large benchmark datasets. We show that with the ability to control sparsity, the proposed Sparse AMM yields superior classification accuracy to the original 
 AMM while simultaneously achieving computational speedup.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://link.springer.com/chapter/10.1007%2F978-3-319-31753-3_3" class="label label-info">PDF</a>
  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://approximateinference.org/accepted/NguyenEtAl2015.pdf">
    <img src="./img/paper/NIPS2015_AABI.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>V Nguyen</strong>, D Phung, T Le, S Venkatesh   <br>
   <strong>Large Sample Asymptotic for Nonparametric Mixture Model with Count Data.</strong><br>
         <i>Workshop on Advances in Approximate Bayesian Inference at Neural Information Processing Systems</i>, (<strong>NIPS</strong>), 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractLSA" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractLSA" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Large Sample Asymptotic for Nonparametric Mixture Model with Count Data</h4>
            </div>
            <div class="modal-body">
Bayesian nonparametric models have become popular recently due to its flexibility
in identifying the unknown number of clusters. However, the flexibility comes at
a cost for learning. Thus, Small Variance Asymptotic (SVA) is one of the promising
approach for scalability in Bayesian nonparametric models. SVA approach for
count data is also developed in which the likelihood function is replaced by the
Kullback–Leibler divergence. In this paper, we present the Large Sample Asymptotic
for count data when the number of sample in Multinomial distribution goes
to infinity, we derive the similar result to SVA for scalable clustering.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://approximateinference.org/accepted/NguyenEtAl2015.pdf" class="label label-info">PDF</a>
	  <a href="https://github.com/ntienvu/LargeSampleAsymptotic_ScalableNonparametricClustering" class="label label-success">Code</a>
	  <a href="http://prada-research.net/~tienvu/slide/nips_aabi15_workshop.pdf" class="label label-warning">Poster</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://learningsys.org/papers/LearningSys_2015_paper_5.pdf">
    <img src="./img/paper/NIPS2015_AABI.jpg" alt="">
  </a>
  <div class="media-body">
   T.D. Nguyen, <strong>V. Nguyen</strong>, T. Le, D. Phung   <br>
   <strong>Sparkling Vector Machines.</strong><br>
         <i>Workshop on Machine Learning Systems at Neural Information Processing Systems</i>, (<strong>NIPS</strong>), 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractSkVM" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractSkVM" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Sparkling Vector Machines</h4>
            </div>
            <div class="modal-body">
Support vector machines (SVMs) are widely-used for classification task in literature.
A data augmentation algorithm is proposed to improve the learning of the
machinery. Distributed SVMs are well-studied, but the distributed implementation
for SVM with data augmentation has not been explored. This paper introduces
a distributed version called sparkling vector machine which is implemented in
Apache Spark, a recent advanced platform for distributed computing. We demonstrate
the scalability of our proposed method on large-scale datasets with hundreds
of million data points. The experimental results show that the predictive performances
of our method are better than or comparable with those of baselines whilst
the execution time is orders of magnitude lower
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://learningsys.org/papers/LearningSys_2015_paper_5.pdf" class="label label-info">PDF</a>
		  <a href="https://github.com/ntienvu/skvm" class="label label-success">Code</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://dro.deakin.edu.au/eserv/DU:30079715/nguyen-bayesianmultilevel-2015A.pdf">
    <img src="./img/paper/rsz_thesis-clipart-article.png" alt="">
  </a>
  <div class="media-body">
   <strong>V. Nguyen</strong><br>
   <strong>Bayesian Nonparametric Multilevel Modelling and Applications</strong><br>
        <i>Deakin University.</i>, Thesis, 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractThesis" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractThesis" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Nonparametric Multilevel Modelling and Applications</h4>
            </div>
            <div class="modal-body">

            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://dro.deakin.edu.au/eserv/DU:30079715/nguyen-bayesianmultilevel-2015A.pdf" class="label label-info">PDF</a>
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://link.springer.com/article/10.1007/s40745-015-0030-3">
    <img src="./img/paper/ADS2015.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>Nguyen, V.</strong>, Phung, D., Pham, D. S., & Venkatesh, S   <br>
       <strong>Bayesian Nonparametric Approaches to Abnormality Detection in Video Surveillance.</strong><br>
         <i>Annals of Data Science</i>, pp 1-21, 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractBayesAbnormal" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractBayesAbnormal" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Nonparametric Approaches to Abnormality Detection in Video Surveillance</h4>
            </div>
            <div class="modal-body">
In data science, anomaly detection is the process of identifying the items, events or observations which do not conform to expected patterns in a dataset. As widely acknowledged 
in the computer vision community and security management, discovering suspicious events is the key issue for abnormal detection in video surveillance. The important steps in identifying 
such events include stream data segmentation and hidden patterns discovery. However, the crucial challenge in stream data segmentation and hidden patterns discovery are the number of coherent 
segments in surveillance stream and the number of traffic patterns are unknown and hard to specify. Therefore, in this paper we revisit the abnormality detection problem through the lens of 
Bayesian nonparametric (BNP) and develop a novel usage of BNP methods for this problem. In particular, we employ the Infinite Hidden Markov Model and Bayesian Nonparametric Factor Analysis for 
stream data segmentation and pattern discovery. 
In addition, we introduce an interactive system allowing users to inspect and browse suspicious events.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://link.springer.com/article/10.1007/s40745-015-0030-3" class="label label-info">PDF</a>
		  <a href="https://github.com/ntienvu/abnormal_detection_video_surveillance" class="label label-success">Code</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://link.springer.com/chapter/10.1007/978-3-319-18038-0_26">
    <img src="./img/paper/PAKDD2015.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>Nguyen, V.</strong>, Phung, D., Venkatesh, S., & Bui, H. H.   <br>
       <strong>A Bayesian Nonparametric Approach to Multilevel Regression.</strong><br>
         <i>Advances in Knowledge Discovery and Data Mining</i> (PAKDD), pp 330-342, 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractMulReg" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractMulReg" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">A Bayesian Nonparametric Approach to Multilevel Regression</h4>
            </div>
            <div class="modal-body">
Regression is at the cornerstone of statistical analysis. Multilevel regression, on the other hand, receives little research attention, though it is prevalent in economics, biostatistics 
and healthcare to name a few. We present a Bayesian nonparametric framework for multilevel regression where individuals including observations and outcomes are organized into groups. 
Furthermore, our approach exploits additional group-specific context observations, we use Dirichlet Process with product-space base measure in a nested structure to model group-level context 
distribution and the regression distribution to accommodate the multilevel structure of the data. The proposed model simultaneously partitions groups into cluster and perform regression. 
We provide collapsed Gibbs sampler for posterior inference.
 We perform extensive experiments on econometric panel data and healthcare longitudinal data to demonstrate the effectiveness of the proposed model.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://link.springer.com/chapter/10.1007/978-3-319-18038-0_26" class="label label-info">PDF</a>
		 <a href="http://prada-research.net/~tienvu/slide/pakdd_2015.pdf" class="label label-warning">Slide</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://prada-research.net/~tienvu/publications/vu_jds_main_camera_ready_2.pdf">
    <img src="./img/paper/JDS2015.jpg" alt="">
  </a>
  <div class="media-body">
     <strong>V Nguyen</strong>, D. Phung, S. Venkatesh   <br>
    <strong>Topic Model Kernel Classiﬁcation With Probabilistically Reduced Features.</strong><br>
         <i>Journal of Data Science</i> pp 323-340, 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractTMKJDS" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractTMKJDS" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Topic Model Kernel Classiﬁcation With Probabilistically Reduced Features</h4>
            </div>
            <div class="modal-body">
Probabilistic topic models have become a standard in modern
machine learning to deal with a wide range of applications. Representing
data by dimensional reduction of mixture proportion extracted
from topic models is not only richer in semantics interpretation,
but could also be informative for classification tasks. In
this paper, we describe the Topic Model Kernel (TMK), a topicbased
kernel for Support Vector Machine classification on data being
processed by probabilistic topic models. The applicability of
our proposed kernel is demonstrated in several classification tasks
with real world datasets. TMK outperforms existing kernels on
the distributional features and give comparative results on nonprobabilistic
data types.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://prada-research.net/~tienvu/publications/vu_jds_main_camera_ready_2.pdf" class="label label-info">PDF</a>
		 <a href="https://github.com/ntienvu/TopicModelKernel" class="label label-success">Code</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="https://sites.google.com/site/iwprha2/proceedings">
    <img src="./img/paper/ICPR2014.jpg" alt="">
  </a>
  <div class="media-body">
   W. Luo, D. Phung, <strong>V Nguyen</strong>, T. Tran and S. Venkatesh   <br>
       <strong>Speed up health research through topic modeling of coded clinical data.</strong><br>
         <i>2nd International Workshop on Pattern Recognition for Healthcare Analytics</i> (IWPRHA), 2014.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICPR14" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICPR14" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Speed up health research through topic modeling of coded clinical data</h4>
            </div>
            <div class="modal-body">
Although random control trial is the gold standard in medical research, researchers are increasingly looking to alternative data sources for hypothesis generation and early-stage
evidence collection. Coded clinical data are collected routinely in most hospitals. While they contain rich information directly related to the real clinical setting, they are both noisy and
semantically diverse, making them difficult to analyze with conventional statistical tools. This paper presents a novel application of Bayesian nonparametric modeling to uncover latent
information in coded clinical data. For a patient cohort, a Bayesian nonparametric model is used to reveal the common comorbidity groups shared by the patients and the proportion
that each comorbidity group is reflected inindividual patient. To demonstrate the method, we present a case study based on hospitalization coding from an Australian hospital. The
model recovered 15 comorbidity groups among 1012 patients hospitalized during a month. When patients from two areas of unequal socio-economic status were compared, it reveals higher
prevalence of diverticular disease in the region of lower socio-economic status. The study builds a convincing case for using routine coded data to speed up hypothesis generation.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="https://sites.google.com/site/iwprha2/proceedings" class="label label-info">PDF</a>

  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="http://jmlr.org/proceedings/papers/v32/nguyenb14.html">
    <img src="./img/paper/ICML2014.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>V Nguyen</strong>, D. Phung, L. Nguyen, S. Venkatesh and H. Bui   <br>
       <strong>Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts.</strong><br>

         <i>Proceedings of The 31st International Conference on Machine Learning</i> (<strong>ICML</strong>), pp. 288–296, 2014.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICML14" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICML14" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts</h4>
            </div>
            <div class="modal-body">
We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents 
and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context 
observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: 
integrating out all contents results in the DPM over contexts, whereas integrating out group-speciﬁc contexts results in the nDP mixture over content variables. We provide a Polya-urn view of 
the model and an efﬁcient collapsed Gibbs inference procedure. 
Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://jmlr.org/proceedings/papers/v32/nguyenb14.html" class="label label-info">PDF</a>
		 <a href="http://prada-research.net/~tienvu/slide/ICML_2014.pdf" class="label label-warning">Slide</a>


  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6529821&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6529821">
    <img src="./img/paper/ISSNIP2013.jpg" alt="">
  </a>
  <div class="media-body">
     <strong>T.V. Nguyen</strong>, D. Phung, S. K. Gupta, and S Venkatesh  <br>
    <strong>Interactive Browsing System for Anomaly Video Surveillance.</strong><br>
         <i>IEEE Eighth International Conference on Intelligent Sensors, Sensor Networks and Information Processing</i> (ISSNIP), pp 384-389, 2013.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractISSNIP13" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractISSNIP13" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Interactive Browsing System for Anomaly Video Surveillance</h4>
            </div>
            <div class="modal-body">
Existing anomaly detection methods in video surveillance exhibit lack of congruence between rare events detected by algorithms and what is considered anomalous by users. 
This paper introduces a novel browsing model to address this issue, allowing users to interactively examine rare events in an intuitive manner. Introducing a novel way to compute rare 
motion patterns, we estimate latent factors of foreground motion patterns through Bayesian Nonparametric Factor analysis. Each factor corresponds to a typical motion pattern. 
A rarity score for each factor is computed, and ordered in decreasing order of rarity, permitting users to browse events using any proportion of rare factors. Rare events correspond to frames
 that contain the rare factors chosen. We present the user with an interface to inspect events that incorporate these rarest factors in a spatial-temporal manner.
 We demonstrate the system on a public video data set, showing key aspects of the browsing paradigm.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6529821&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6529821" class="label label-info">PDF</a>
		  <a href="https://github.com/ntienvu/abnormal_detection_video_surveillance" class="label label-success">Code</a>
		<a href="http://prada-research.net/~tienvu/slide/issnip13_poster.pdf" class="label label-warning">Poster</a>


  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="http://prada-research.net/~tienvu/publications/phung_etal_tr12.pdf">
    <img src="./img/paper/ISSNIP2013.jpg" alt="">
  </a>
  <div class="media-body">
   Phung, D., Nguyen, X., Bui, H., <strong>Nguyen, T.V.</strong> and Venkatesh, S  <br>
   <strong>Conditionally Dependent Dirichlet Processes for Modelling Naturally Correlated Data Sources.</strong><br>
         <i>Technical Report</i>, Deakin University, 2012.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractTR2012" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractTR2012" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Conditionally Dependent Dirichlet Processes for Modelling Naturally Correlated Data Sources</h4>
            </div>
            <div class="modal-body">
We introduce a new class of conditionally dependent Dirichlet processes (CDP) for hierarchical
mixture modelling of naturally correlated data sources. This class of models provides a Bayesian
nonparametric approach for modelling a range of challenging datasets which typically consists of
heterogeneous observations from multiple correlated data channels. Some typical examples include
annotated social media, networks in community where information about friendship and relation
coexist with user’s profiles, medical records where patient’s information exists in several dimension
(demographic information, medical history, drug uses and so on). The proposed framework can
easily be tailored to model multiple data sources which are correlated by some latent underlying
processes, whereas most of existing topic models, notably hierarchical Dirichlet processes (HDP), is
designed for only a single data observation channel. In these existing approaches, data are grouped
into documents (e.g., text documents or they are grouped according to some covariates such as time
or location). Our approach is different: we view context as distributions over some index space and
model both topics and contexts jointly. Distributions over topic parameters are modelled according
to the usual Dirichlet processes. Stick-breaking representation gives rise to explicit realizations of
topic atoms which we use as an indexing mechanism to induce conditional random mixture distributions
on the context observation spaces – loosely speaking, we use a stochastic process, being DP,
to conditionally ‘index’ other stochastic processes. The later can be designed on any suitable family
of stochastic processes to suit modelling needs or data types of contexts (such as Beta or Gaussian
processes). Dirichlet process is of course an obvious choice. Our model can be viewed as an integration
of the hierarchical Dirichlet process (HDP) and the recent nested Dirichlet process (nDP) with shared mixture components. In fact, it provides an interesting interpretation whereas, under
a suitable parameterization, integrating out the topic components results in a nested DP, whereas
integrating out the context components results in a hierarchical DP. Different approaches for posterior
inference exist. This paper focus on the development of an auxiliary conditional Gibbs sampling
in which both topic and context atoms are marginalized out. We demonstrate the framework on
synthesis datasets for temporal topic modelling and trajectory discovery in videos surveillance. We
then demonstrate an application on a current visual category classification challenge in computer
vision for which we significantly outperform the current reported state-of-the-art results. Finally,
it is worthwide to note that our proposed approach can be easily twisted to accommodate different
forms of supervision (weakly annotated data, semi-supervision) and to perform prediction.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://prada-research.net/~tienvu/publications/phung_etal_tr12.pdf" class="label label-info">PDF</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6460383&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F6425799%2F6460043%2F06460383.pdf%3Farnumber%3D6460383">
    <img src="./img/paper/ICPR2012.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>T.V. Nguyen</strong>, D. Phung, S. Rana, DS Pham, and S Venkatesh  <br>
   <strong>Multi-model abnormality detection in video with unknown data segmentation.</strong><br>
         <i>21st International Conference on Pattern Recognition</i> (ICPR), pp 1322-1325, 2012.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICPR12" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICPR12" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Multi-model abnormality detection in video with unknown data segmentation</h4>
            </div>
            <div class="modal-body">
This paper examines a new problem in large scale stream data: abnormality detection which is localized to a data segmentation process. Unlike traditional abnormality detection methods which 
typically build one unified model across data stream, we propose that building multiple detection models focused on different coherent sections of the video stream would result in better detection
 performance. One key challenge is to segment the data into coherent sections as the number of segments is not known in advance and can vary greatly across cameras; and a principled way approach 
 is required. To this end, we first employ the recently proposed infinite HMM and collapsed Gibbs inference to automatically infer data segmentation followed by constructing abnormality detection 
 models which are localized to each segmentation.
 We demonstrate the superior performance of the proposed framework in a real-world surveillance camera data over 14 days.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6460383&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F6425799%2F6460043%2F06460383.pdf%3Farnumber%3D6460383" class="label label-info">PDF</a>
		  <a href="https://github.com/ntienvu/abnormal_detection_video_surveillance" class="label label-success">Code</a>

  </div>
</div>


		</div>
      </div>
		</div>
      </div>
      </div>
      </div>
      <footer class="text-center text-muted">
        <hr>
        Last updated Sep 05, 2018.<br>
	Based on the code of 
        <a href="https://github.com/alopez/alopez.github.com">Adam Lopez</a>.<br>
        Created with 
        <a href="http://git-scm.com/">git</a>,
        <a href="http://jekyllrb.com/">jekyll</a>,
        <a href="http://getbootstrap.com/">bootstrap</a>,
        and <a href="http://www.vim.org/">vim</a>.<br> 
        <br><br>
      </footer>
    </div>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
         m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-46770577-1', 'jhu.edu');
      ga('send', 'pageview');
    </script>
  


</body></html>

