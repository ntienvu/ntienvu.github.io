<!DOCTYPE html>
<!-- saved from url=(0050)http://people.eng.unimelb.edu.au/tcohn/papers.html -->
<html class="gr__people_eng_unimelb_edu_au"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>Vu Nguyen | Publications</title>

    <link href="./css/bootstrap.min.css" rel="stylesheet" media="screen">
    <link href="./css/bootstrap-glyphicons.css" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- MathJax -->
    <script async="" src="./js/analytics.js"></script><script type="text/javascript" src="./js/MathJax.js">
    </script>

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  <style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style></head>
  <body data-gr-c-s-loaded="true"><div id="MathJax_Message" style="display: none;"></div>
    <!-- JavaScript plugins (requires jQuery) -->
    <script src="./js/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./js/bootstrap.min.js"></script>

    <div class="container">
      <div class="row">
        <div class="col-sm-2">
          <br><br> 
<img src="./img/person/Vu_Avatar_2019.jpg" class="img-responsive" alt="bruge">
<ul class="nav navbar-inverse">
  <li>
    <a href="index.html">Home</a>
  </li>
  <li>
    <a href="publications.html">Publications</a>
  </li>
    <li>
    <a href="CV.html">CV</a>
  </li>

</ul>


        </div>
        <div class="col-sm-10">
          <div class="page-header">
		  
  
	<h2>Journal and Conference Papers</h2>	  
</div>

  <h3>2020</h3>



  <div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
<strong>V. Nguyen</strong>, V. Masrani, R. Brekelmans, M. A. Osborne, F. Wood  <br>
	    <strong>Gaussian Process Bandit Optimization of the Thermodynamic Variational Objective
</strong><br>
         <i>Neural Information Processing Systems</i> (<strong>NeurIPS</strong>), accepted, 2020.<br>
       
	  
      <a data-toggle="modal" href="#abstractTVO" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractP2BT" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Gaussian Process Bandit Optimization of the Thermodynamic Variational Objective
</h4>
            </div>
            <div class="modal-body">
			Achieving the full promise of the Thermodynamic Variational Objective (TVO), a recently proposed variational inference objective that lower-bounds the log evidence via one-dimensional Riemann integration, requires choosing a ``schedule'' of sorted discretization points. This paper introduces a bespoke Gaussian process bandit optimization method for automatically choosing these points. Our approach not only automates their one-time selection, but also dynamically adapts their positions over the course of optimization, leading to improved model learning and inference. We provide theoretical guarantees that our bandit optimization converges to the regret-minimizing choice of integration points. Empirical validation of our algorithm is provided in terms of improved learning and inference in Variational Autoencoders and sigmoid belief networks.
          </div><!-- /.modal-content -->

        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 

  </div>
</div>



				  <br>


  <div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/boil.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
 <strong>V. Nguyen*</strong>, S. Schulze*, M. A. Osborne  <br>
	    <strong>Bayesian Optimisation for Iterative Learning
</strong><br>
         <i>Neural Information Processing Systems</i> (<strong>NeurIPS</strong>), accepted, 2020.<br>
         <i>Preliminary version at 7th AutoML Workshop at International Conference on Machine Learning</i> (<strong>ICML</strong>), 2020.<br>
				  
      <a data-toggle="modal" href="#abstractBOIL" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractBOIL" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Optimization for Iterative Learning
</h4>
            </div>
            <div class="modal-body">
The success of deep (reinforcement) learning systems crucially depends on the correct choice of hyperparameters which are notoriously sensitive and expensive to evaluate. Training these systems typically requires running iterative processes over multiple epochs or episodes. Traditional approaches only consider final performances of a hyperparameter although intermediate information from the learning curve is readily available. In this paper, we present a Bayesian optimization approach which exploits the iterative structure of learning algorithms for efficient hyperparameter tuning. First, we transform each training curve into a numeric score. Second, we selectively augment the data using the auxiliary information from the curve. This augmentation step enables modeling efficiency while preventing the ill-conditioned issue of Gaussian process covariance matrix happened when adding the whole curve. We demonstrate the efficiency of our algorithm by tuning hyperparameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://arxiv.org/abs/1909.09593" class="label label-info">PDF</a>
		<a href="https://slideslive.com/38930629/bayesian-optimization-for-iterative-learning" class="label label-danger">Talk</a>

  </div>
</div>



  <div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
 J. Parker-Holder, <strong>V. Nguyen</strong>, S. Roberts  <br>
	    <strong>Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits
</strong><br>
         <i>Neural Information Processing Systems</i> (<strong>NeurIPS</strong>), accepted, 2020.<br>
         <i>Preliminary version at 7th AutoML Workshop at International Conference on Machine Learning</i> (<strong>ICML</strong>), 2020.<br>
				  
      <a data-toggle="modal" href="#abstractP2BT" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractP2BT" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits
</h4>
            </div>
            <div class="modal-body">
Selecting optimal hyperparameters is a key challenge in machine learning. A recent approach to this problem (PBT) showed it is possible to achieve impressive performance by updating both weights and hyperparameters in a single training run of a population of agents. Despite it's success, PBT relies on heuristics to explore the hyperparameter space, thus lacks theoretical guarantees, requires vast computational resources and often suffers from mode collapse when this is not available. In this work we introduce Population-Based Bandits (PB2), the first provably efficient PBT-style algorithm. PB2 uses a probabilistic model to balance exploration and exploitation, thus it is able to discover high performing hyperparameter configurations with far fewer agents than typically required by PBT.</div>
          </div><!-- /.modal-content -->

        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://arxiv.org/pdf/2002.02518.pdf" class="label label-info">PDF</a>
	  		  <a href="https://github.com/jparkerholder/PB2" class="label label-success">Code</a>
		<a href="https://slideslive.com/38930622/provably-efficient-online-hyperparameter-optimization-via-populationbased-bandits" class="label label-danger">Contributed Talk [3% selected]</a>

	  
  </div>
</div>



  <div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/icml20_kov.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
  	<strong>V. Nguyen</strong>, M. A. Osborne <br>
	    <strong>Knowing The What But Not The Where in Bayesian Optimization
</strong><br>

		   <i>International Conference on Machine Learning</i> (<strong>ICML</strong>), 2020.<br>
				   
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractERM2019" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractERM2019" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Knowing The What But Not The Where in Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization has demonstrated impressive success in finding the optimum location x* and value f*=f(x*)=max f(x) of the black-box function f. 
			In some applications, however, the optimum value is known in advance and the goal is to find the corresponding optimum location. 
			Existing work in Bayesian optimization (BO) has not effectively exploited the knowledge of f* for optimization. 
			In this paper, we consider a new setting in BO in which the knowledge of the optimum value is available. Our goal is to exploit the knowledge about f* to search for the location x* efficiently. 
			To achieve this goal, we first transform the Gaussian process surrogate using the information about the optimum value. 
			Then, we propose two acquisition functions, called confidence bound minimization and expected regret minimization, which exploit the knowledge about the optimum value to identify the optimum location efficiently. 
			We show that our approaches work both intuitively and quantitatively achieve better performance against standard BO methods. 
			We demonstrate real applications in tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on Skin Segmentation dataset in which the optimum values are publicly available.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://arxiv.org/pdf/1905.02685.pdf" class="label label-info">PDF</a>
		  <a href="https://github.com/ntienvu/KnownOptimum_BO" class="label label-success">Code</a>
		<a href="https://slideslive.com/38927867/knowing-the-what-but-not-the-where-in-bayesian-optimization" class="label label-danger">Talk</a>

	  
  </div>
</div>



  
  <div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/cocabo.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
  B. Ru, AS. Alvi,	<strong>V. Nguyen</strong>, M. A. Osborne, SJ. Roberts  <br>
	    <strong>Bayesian Optimisation over Multiple Continuous and Categorical Inputs
</strong><br>

		   <i>International Conference on Machine Learning</i> (<strong>ICML</strong>), 2020.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractNeurIPS19" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractNeurIPS19" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Optimisation over Multiple Continuous and Categorical Inputs</h4>
            </div>
            <div class="modal-body">
			Efficient optimisation of black-box problems that comprise both continuous and categorical inputs is important, yet poses significant challenges. We propose a new approach, Continuous and Categorical Bayesian Optimisation (CoCaBO), which combines the strengths of multi-armed bandits and Bayesian optimisation to select values for both categorical and continuous inputs. We model this mixed-type space using a Gaussian Process kernel, designed to allow sharing of information across multiple categorical variables, each with multiple possible values; this allows CoCaBO to leverage all available data efficiently. We extend our method to the batch setting and propose an efficient selection procedure that dynamically balances exploration and exploitation whilst encouraging batch diversity. We demonstrate empirically that our method outperforms existing approaches on both synthetic and real-world optimisation tasks with continuous and categorical inputs.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://arxiv.org/abs/1906.08878" class="label label-info">PDF</a>
		  <a href="https://github.com/rubinxin/CoCaBO_code" class="label label-success">Code</a>
		<a href="https://slideslive.com/38927978/bayesian-optimisation-over-multiple-continuous-and-categorical-inputs" class="label label-danger">Talk</a>

	  
  </div>
</div>

  



  <div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/vae_nina.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
 N. van Esbroeck,  D. Lennon, H. Moon, <strong>V. Nguyen</strong>, F. Vigneau, LC. Camenzind, L. Yu, DM. Zumbühl, G. A. D. Briggs,  D. Sejdinovic, N. Ares  <br>
	    <strong>Quantum device fine-tuning using unsupervised embedding learning
</strong><br>

         New Journal of Physics 2020.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractQuantumVAE" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractQuantumVAE" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Quantum device fine-tuning using unsupervised embedding learning
</h4>
            </div>
            <div class="modal-body">
			Quantum devices with a large number of gate electrodes allow for precise control of device
parameters. This capability is hard to fully exploit due to the complex dependence of these
parameters on applied gate voltages. We experimentally demonstrate an algorithm capable of
fine-tuning several device parameters at once. The algorithm acquires a measurement and assigns
it a score using a variational auto-encoder. Gate voltage settings are set to optimise this score in
real-time in an unsupervised fashion. We report fine-tuning times of a double quantum dot device
within approximately 40 min.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
	  <a href="https://iopscience.iop.org/article/10.1088/1367-2630/abb64c/pdf" class="label label-info">PDF</a>

  </div>
</div>






<div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/kbs20.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
  	J. Berk, S. Gupta, S. Rana, <strong>V. Nguyen</strong>,  S. Venkatesh <br>
	    <strong>Bayesian optimisation in unknown bounded search domains</strong><br>

         <i>Knowledge-Based Systems
</i>,  pp. 621-637, Elsevier, 105645, 2020.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractECML18" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractECML18" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian optimisation in unknown bounded search domains</h4>
            </div>
            <div class="modal-body">
			Bayesian optimisation (BO) is one of the most sample efficient methods for determining the optima of expensive, noisy black-box functions. Despite its tremendous success in scientific discovery and hyperparameter tuning, it still requires a bounded search space. The search spaces boundaries are, however, often chosen heuristically with an educated guess. If the boundaries are misspecified, then the search space may either be unnecessarily large and hence more expensive to optimise, or it may simply not contain the global optimum. In this paper, we introduce a method for dynamically determining the bound directly from the data. This is done using a distribution of the bound derived in a Bayesian setting. The prior is chosen by the user and the likelihood is derived with Thompson sampling. This results in a bound that is both cheap to optimise and has a high probability of containing the global optimum. We compare the performance of our method with the alternative methods on a range of synthetic and real-world problems and demonstrate that our method achieves consistently superior results.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://www.sciencedirect.com/science/article/abs/pii/S095070512030099X" class="label label-info">PDF</a>
	  <a href="https://github.com/jmaberk/DDB" class="label label-success">Code</a>

	  
  </div>
</div>





  <h3>2019</h3>
  
  
  <div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/DRL_quantum.png" alt=""  width="140" height="100">
  </a>
  <div class="media-body">
 <strong>V. Nguyen</strong>, D. Lennon, H. Moon, N. Esbroeck, D. Sejdinovic, M. A. Osborne, G. A. D. Briggs, N. Ares  <br>
	    <strong>Controlling Quantum Device Measurement using Deep Reinforcement Learning
</strong><br>

         <i>Deep Reinforcement Learning workshop,</i> NeurIPS 2019.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractDRL" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractDRL" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Controlling Quantum Device Measurement using Deep Reinforcement Learning
</h4>
            </div>
            <div class="modal-body">
			Qubits based on semiconductor quantum dot devices are promising building blocks for the realisation of
quantum computers. However, measuring and characterising these quantum dot devices can be challenging and laborious for the experimentalists. In this paper, we develop an elegant application using deep
reinforcement learning for controlling the measurement of quantum dot devices. Specifically, we present
a computer-automated algorithm that measures a map of current flowing through a double quantum
dot device for different settings of its gate electrodes. The algorithm seeks particular features called
bias-triangles indicating the device is in the right operating regime of realising a qubit. Our approach
requires no human intervention and significantly reduces the measurement time. This work alleviates
the user effort required to measure multiple quantum dot devices, each with multiple gate electrodes.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
	  <a href="papers/DRL_Quantum_Short.pdf" class="label label-info">PDF</a>
	  <a href="papers/DRL_Quantum_Long.pdf" class="label label-info">Long Version</a>

	  
  </div>
</div>


 
  <div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/continual_learning.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
 	S. Kessler, <strong>V. Nguyen</strong>, S. Zohren, S. Robert   <br>
	    <strong>Indian Buffet Neural Networks for Continual Learning
</strong><br>

         <i>Bayesian Deep Learning workshop,</i> NeurIPS 2019.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractIBNN" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractIBNN" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Indian Buffet Neural Networks for Continual Learning
</h4>
            </div>
            <div class="modal-body">
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
	  <a href="https://arxiv.org/abs/1912.02290" class="label label-info">PDF</a>

	  
  </div>
</div>


<br>

  









 <div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/pvrs.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
 <strong>V. Nguyen</strong>, S. Gupta, S. Rana, M. Thai, C. Li, S. Venkatesh  <br>
	    <strong>Efficient Bayesian Optimization for Uncertainty Reduction over Perceived Optima Locations
</strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>), pp. 1270-1275, 2019. [194/1046=18%]<br> 
		  Preliminary version appears at <i>NIPS Workshop on Bayesian Optimization, (<strong>NIPSW</strong>), 2017.</i> <br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractPVRS" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractPVRS" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Efficient Bayesian Optimization for Uncertainty Reduction over Perceived Optima Locations
</h4>
            </div>
            <div class="modal-body">
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  <a href="https://ieeexplore.ieee.org/abstract/document/8970715" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ICDM2019_PVRS" class="label label-success">Code</a>

	  
  </div>
</div>






  <h3>2018</h3>


<div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
  	S. Gopakumar, S. Gupta, S. Rana, <strong>V. Nguyen</strong>, S. Venkatesh <br>
	    <strong>Algorithmic Assurance: An Active Approach to Algorithmic Testing using Bayesian Optimisation</strong><br>

         <i>Advances in Neural Information Processing Systems</i>, (<strong>NeurIPS</strong>), pp. 5470-5478, 2018.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractNIPS18" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractNIPS18" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Algorithmic Assurance: An Active Approach to Algorithmic Testing using Bayesian Optimisation</h4>
            </div>
            <div class="modal-body">
			We introduce algorithmic assurance, the problem of testing whether machine learning algorithms are conforming to their intended design goal. We address this problem by proposing an efficient framework for algorithmic testing. To provide assurance, we need to efficiently discover scenarios where an algorithm decision deviates maximally from its intended gold standard. We mathematically formulate this task as an optimisation problem of an expensive, black-box function. We use an active learning approach based on Bayesian optimisation to solve this optimisation problem. We extend this framework to algorithms with vector-valued outputs by making appropriate modification in Bayesian optimisation via the EXP3 algorithm. We theoretically analyse our methods for convergence. Using two real-world applications, we demonstrate the efficiency of our methods. The significance of our problem formulation and initial solutions is that it will serve as the foundation in assuring humans about machines making complex decisions.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="http://papers.nips.cc/paper/7791-algorithmic-assurance-an-active-approach-to-algorithmic-testing-using-bayesian-optimisation.pdf" class="label label-info">PDF</a>
	  <a href="https://github.com/shivapratap/AlgorithmicAssurance_NIPS2018" class="label label-success">Code</a>

	  
  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/rsz_3dprinting.jpg" alt="">
  </a>
  <div class="media-body">
  	C. Li, S. Rana, S. Gupta, <strong>V. Nguyen</strong>, S. Venkatesh, A. Sutti, D. Rubin, T. Slezak, M. Height, M. Mohammed, and I. Gibson <br>
	    <strong>Accelerating Experimental Design by Incorporating Experimenter Hunches</strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>), pp. 257-266, 2018. [84/948=9%]<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICDM18" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICDM18" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Accelerating Experimental Design by Incorporating Experimenter Hunches</h4>
            </div>
            <div class="modal-body">
			Experimental design is a process of obtaining a product with target property via experimentation. Bayesian optimization offers a sample-efficient tool for experimental design when experiments are expensive. Often, expert experimenters have 'hunches' about the behavior of the experimental system, offering potentials to further improve the efficiency. In this paper, we consider per-variable monotonic trend in the underlying property that results in a unimodal trend in those variables for a target value optimization. For example, sweetness of a candy is monotonic to the sugar content. However, to obtain a target sweetness, the utility of the sugar content becomes a unimodal function, which peaks at the value giving the target sweetness and falls off both ways. In this paper, we propose a novel method to solve such problems that achieves two main objectives: a) the monotonicity information is used to the fullest extent possible, whilst ensuring that b) the convergence guarantee remains intact. This is achieved by a two-stage Gaussian process modeling, where the first stage uses the monotonicity trend to model the underlying property, and the second stage uses `virtual' samples, sampled from the first, to model the target value optimization function. The process is made theoretically consistent by adding appropriate adjustment factor in the posterior computation, necessitated because of using the `virtual' samples. The proposed method is evaluated through both simulations and real world experimental design problems of a) new short polymer fiber with the target length, and b) designing of a new three dimensional porous scaffolding with a target porosity. In all scenarios our method demonstrates faster convergence than the basic Bayesian optimization approach not using such `hunches'.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594850" class="label label-info">PDF</a>
	  <a href="	  https://www.dropbox.com/s/nh6jgjktuluxwj6/ICDM2018_monotonicity_BO.zip?dl=0" class="label label-success">Code</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
  	J. Berk, <strong>V. Nguyen</strong>, S. Gupta, S. Rana, S. Venkatesh <br>
	    <strong>Exploration Enhanced Expected Improvement for Bayesian Optimization</strong><br>

         <i>European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</i>, (<strong>ECML-PKDD</strong>), pp. 621-637, 2018.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractECML18" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractECML18" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Exploration Enhanced Expected Improvement for Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization (BO) is a sample-efficient method for global
optimization of expensive, noisy, black-box functions using probabilistic methods. The performance of a BO method depends on its selection strategy through
an acquisition function. This must balance improving our understanding of the
function in unknown regions (exploration) with locally improving on known promising samples (exploitation). Expected improvement (EI) is one of the most widely
used acquisition functions for BO. Unfortunately, it has a tendency to over-exploit,
meaning that it can be slow in finding new peaks. We propose a modification to EI
that will allow for increased early exploration while providing similar exploitation once the system has been suitably explored. We also prove that our method
has a sub-linear convergence rate and test it on a range of functions to compare
its performance against the standard EI and other competing methods.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://link.springer.com/chapter/10.1007/978-3-030-10928-8_37" class="label label-info">PDF</a>
	  <a href="https://github.com/jmaberk/BO_with_E3I" class="label label-success">Code</a>

	  
  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="https://www.ijcai.org/proceedings/2018/0434.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
  	X. Zhang, W. Li, <strong>V. Nguyen</strong>, F. Zhuang, H. Xiong, S. Lu <br>
	    <strong>Label-Sensitive Task Grouping by Bayesian Nonparametric Approach for Multi-Task Multi-Label Learning</strong><br>

         <i>International Joint Conference on Artificial Intelligence</i>, (<strong>IJCAI</strong>), pp. 3125-3131,   2018.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractIJCAI18" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractIJCAI18" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Label-Sensitive Task Grouping by Bayesian Nonparametric Approach for Multi-Task Multi-Label Learning</h4>
            </div>
            <div class="modal-body">
			Multi-label learning is widely applied in many realworld
applications, such as image and gene annotation.
While most of the existing multi-label
learning models focus on the single-task learning
problem, there are always some tasks that share
some commonalities, which can help each other to
improve the learning performances if the knowledge
in the similar tasks can be smartly shared.
In this paper, we propose a LABel-sensitive TAsk
Grouping framework, named LABTAG, based on
Bayesian nonparametric approach for multi-task
multi-label classification. The proposed framework
explores the label correlations to capture featurelabel
patterns, and clusters similar tasks into groups
with shared knowledge, which are learned jointly
to produce a strengthened multi-task multi-label
model. We evaluate the model performance on
three public multi-task multi-label data sets, and the
results show that LABTAG outperforms the compared
baselines with a significant margin.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://www.ijcai.org/proceedings/2018/0434.pdf" class="label label-info">PDF</a>

	  
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="https://link.springer.com/article/10.1007/s10115-018-1238-2">
    <img src="./img/paper/search_space.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Filtering Bayesian Optimization Approach in Weakly Specified Search Space</strong><br>

         <i>Knowledge and Information Systems</i>, (KAIS),    2018.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractKAIS18_BO" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractKAIS18_BO" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Filtering Bayesian Optimization Approach in Weakly Specified Search Space</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization (BO) has recently emerged as a powerful and flexible tool for hyper-parameter tuning and more generally for the efficient global optimization of expensive black-box functions. 
			Systems implementing BO have successfully solved difficult problems in automatic design choices and machine learning hyper-parameters tunings. Many recent advances in the methodologies and theories underlying Bayesian optimization have extended the framework to new applications and provided greater insights into the behavior of these algorithms. Still, these established techniques always require a user-defined space to perform optimization. This pre-defined space specifies the ranges of hyper-parameter values. 
			In many situations, however, it can be difficult to prescribe such spaces, as a prior knowledge is often unavailable. Setting these regions arbitrarily can lead to inefficient optimization—if a space is too large, we can miss the optimum with a limited budget, and on the other hand, if a space is too small, it may not contain the optimum point that we want to get. The unknown search space problem is intractable to solve in practice. 
			Therefore, in this paper, we narrow down to consider specifically the setting of “weakly specified” search space for Bayesian optimization. By weakly specified space, we mean that the pre-defined space is placed at a sufficiently good region so that the optimization can expand and reach to the optimum. However, this pre-defined space need not include the global optimum. 
			We tackle this problem by proposing the filtering expansion strategy for Bayesian optimization. Our approach starts from the initial region and gradually expands the search space. We develop an efficient algorithm for this strategy and derive its regret bound. 
			These theoretical results are complemented by an extensive set of experiments on benchmark functions and two real-world applications which demonstrate the benefits of our proposed approach.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://link.springer.com/article/10.1007/s10115-018-1238-2" class="label label-info">PDF</a>
	  <a href="https://github.com/ntienvu/ICDM2017_FBO" class="label label-success">Code</a>

	  
  </div>
</div>



  <h3>2017</h3>



<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v77/nguyen17a/nguyen17a.pdf">
    <img src="./img/paper/regret.PNG" alt=""  width="140" height="100">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Regret for Expected Improvement over the Best-Observed Value and Stopping Condition</strong><br>

        <i>Proceedings of The 9th Asian Conference on Machine Learning</i>, (<strong>ACML</strong>),  pp. 279-294,    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractACML17" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractACML17" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Regret for Expected Improvement over the Best-Observed Value and Stopping Condition</h4>
            </div>
            <div class="modal-body">
Bayesian optimization (BO) is a sample-efficient method for global optimization of expensive,
noisy, black-box functions using probabilistic methods. The performance of a BO method depends
on its selection strategy through the acquisition function. Expected improvement (EI) is one of
the most widely used acquisition functions for BO that finds the expectation of the improvement
function over the incumbent. The incumbent is usually selected as the best-observed value so far,
termed as y^max (for the maximizing problem). Recent work has studied the convergence rate for
EI under some mild assumptions or zero noise of observations. Especially, the work of Wang and
de Freitas (2014) has derived the sublinear regret for EI under a stochastic noise. However, due to
the difficulty in stochastic noise setting and to make the convergent proof feasible, they use an alternative
choice for the incumbent as the maximum of the Gaussian process predictive mean, \mu^max.
This modification makes the algorithm computationally inefficient because it requires an additional
global optimization step to estimate mmax that is costly and may be inaccurate. To address this
issue, we derive a sublinear convergence rate for EI using the commonly used ymax. Moreover, our
analysis is the first to study a stopping criteria for EI to prevent unnecessary evaluations. Our analysis
complements the results of Wang and de Freitas (2014) to theoretically cover two incumbent
settings for EI. Finally, we demonstrate empirically that EI using y^max is both more computationally
efficiency and more accurate than EI using \mu^max.
			</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="http://proceedings.mlr.press/v77/nguyen17a/nguyen17a.pdf" class="label label-info">PDF</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/8215507/">
    <img src="./img/paper/search_space.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>Bayesian Optimization in Weakly Specified Search Space</strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 347-356,    2017. [72/778=9%]<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICDM17_FBO" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICDM17_FBO" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Optimization in Weakly Specified Search Space</h4>
            </div>
            <div class="modal-body">
Bayesian optimization (BO) has recently emerged
as a powerful and flexible tool for hyper-parameter tuning and
more generally for the efficient global optimization of expensive
black-box functions. Systems implementing BO has successfully
solved difficult problems in automatic design choices and machine
learning hyper-parameters tunings. Many recent advances in
the methodologies and theories underlying Bayesian optimization
have extended the framework to new applications and provided
greater insights into the behavior of these algorithms. Still, these
established techniques always require a user-defined space to
perform optimization. This pre-defined space specifies the ranges
of hyper-parameter values. In many situations, however, it can
be difficult to prescribe such spaces, as a prior knowledge is
often unavailable. Setting these regions arbitrarily can lead to
inefficient optimization - if a space is too large, we can miss the
optimum with a limited budget, on the other hand, if a space is
too small, it may not contain the optimum point that we want to
get. The unknown search space problem is intractable to solve in
practice. Therefore, in this paper, we narrow down to consider
specifically the setting of “weakly specified” search space for
Bayesian optimization. By weakly specified space, we mean that
the pre-defined space is placed at a sufficiently good region so that
the optimization can expand and reach to the optimum. However,
this pre-defined space need not include the global optimum.
We tackle this problem by proposing the filtering expansion
strategy for Bayesian optimization. Our approach starts from
the initial region and gradually expands the search space. We
develop an efficient algorithm for this strategy and derive its
regret bound. These theoretical results are complemented by an
extensive set of experiments on benchmark functions and two
real-world applications which demonstrate the benefits of our
proposed approach.</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://github.com/ntienvu/ICDM2017_FBO" class="label label-success">Code</a>
		<a href="" class="label label-danger">Selected as Best Papers, Invited for KAIS</a>
	  <a href="http://ieeexplore.ieee.org/document/8215507/" class="label label-info">PDF</a>


  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/8215498/">
    <img src="./img/paper/gogp.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
    T. Le, K. Nguyen, <strong>V. Nguyen</strong>, T. D. Nguyen, D. Phung <br>
	    <strong>GoGP: Fast Online Regression with Gaussian Processes</strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 257-266,    2017. [72/778=9%]<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICDM17_GoGP" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICDM17_GoGP" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">GoGP: Fast Online Regression with Gaussian Processes </h4>
            </div>
            <div class="modal-body">
One of the most current challenging problems in Gaussian process regression (GPR) is to handle large-scale datasets and to accommodate an online learning setting where data arrive 
irregularly on the fly. In this paper, we introduce a novel online Gaussian process model that could scale with massive datasets. Our approach is formulated based on alternative 
representation of the Gaussian process under geometric and optimization views, hence termed geometric-based online GP (GoGP). We developed theory to guarantee that with a good convergence 
rate our proposed algorithm always produces a (sparse) solution which is close to the true optima to any arbitrary level of approximation accuracy specified a priori. 
Furthermore, our method is proven to scale seamlessly not only with large-scale datasets, but also to adapt accurately with streaming data. 
We extensively evaluated our proposed model against state-of-the-art baselines using several large-scale datasets for online regression task. 
The experimental results show that our GoGP delivered comparable, or slightly better, predictive performance while achieving a magnitude of computational speedup compared 
with its rivals under online setting. 
More importantly, its convergence behavior is guaranteed through our theoretical analysis, which is rapid and stable while achieving lower errors.
			</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  <a href="https://github.com/ntienvu/GoGP" class="label label-success">Code</a>
	  <a href="" class="label label-danger">Selected as Best Papers, Invited for KAIS</a>
	  <a href="http://ieeexplore.ieee.org/document/8215498/" class="label label-info">PDF</a>

  </div>
</div>

<div class="media">
  <a class="pull-left thumbnail" href="http://www.jmlr.org/papers/v18/16-191.html">
    <img src="./img/paper/avm.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
    T. Le, T. D Nguyen, <strong>V. Nguyen</strong>, D. Phung, <br>
	    <strong>Approximation Vector Machines for Large-scale Online Learning</strong><br>

                   <i>Journal of Machine Learning Research</i>, (<strong>JMLR</strong>), 2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractJMLR17" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractJMLR17" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Approximation Vector Machines for Large-scale Online Learning</h4>
            </div>
            <div class="modal-body">
			One of the most challenging problems in kernel online learning is to bound the model size
and to promote model sparsity. Sparse models not only improve computation and memory
usage, but also enhance the generalization capacity – a principle that concurs with the law
of parsimony. However, inappropriate sparsity modeling may also significantly degrade the
performance. In this paper, we propose Approximation Vector Machine (AVM), a model
that can simultaneously encourage sparsity and safeguard its risk in compromising the performance.
In an online setting context, when an incoming instance arrives, we approximate
this instance by one of its neighbors whose distance to it is less than a predefined threshold.
Our key intuition is that since the newly seen instance is expressed by its nearby neighbor
the optimal performance can be analytically formulated and maintained. We develop
theoretical foundations to support this intuition and further establish an analysis for the
common loss functions including Hinge, smooth Hinge, and Logistic (i.e., for the classifi-
cation task) and `1, `2, and ε-insensitive (i.e., for the regression task) to characterize the
gap between the approximation and optimal solutions. This gap crucially depends on two
key factors including the frequency of approximation (i.e., how frequent the approximation
operation takes place) and the predefined threshold. We conducted extensive experiments
for classification and regression tasks in batch and online modes using several benchmark
datasets. The quantitative results show that our proposed AVM obtained comparable predictive
performances with current state-of-the-art methods while simultaneously achieving
significant computational speed-up due to the ability of the proposed AVM in maintaining
the model size.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
		  <a href="http://www.jmlr.org/papers/volume18/16-191/16-191.pdf" class="label label-info">PDF</a>
			  <a href="https://github.com/ntienvu/avm" class="label label-success">Code</a>


  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v70/rana17a/rana17a.pdf">
    <img src="./img/paper/elastic_gp.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
    S. Rana, C. Li, S. Gupta, <strong>V. Nguyen</strong>, S. Venkatesh <br>
	    <strong>High Dimensional Bayesian Optimization with Elastic Gaussian Process</strong><br>

         <i>Proceedings of the 34th International Conference on Machine Learning</i>, (<strong>ICML</strong>), pp 2883-2891,   2017.<br>

      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICML17" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICML17" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">High Dimensional Bayesian Optimization with Elastic Gaussian Process</h4>
            </div>
            <div class="modal-body">
			Bayesian optimization is an efficient way to optimize expensive black-box functions such as designing a new product with highest quality or hyperparameter tuning of a machine learning algorithm. However, it has a serious limitation when the parameter space is high-dimensional as Bayesian optimization crucially depends on solving a global optimization of a surrogate utility function in the same sized dimensions. The surrogate utility function, known commonly as acquisition function is a continuous function but can be extremely sharp at high dimension - having only a few peaks marooned in a large terrain of almost flat surface. Global optimization algorithms such as DIRECT are infeasible at higher dimensions and gradient-dependent methods cannot move if initialized in the flat terrain. We propose an algorithm that enables local gradient-dependent algorithms to move through the flat terrain by using a sequence of gross-to-finer Gaussian process priors on the objective function as we leverage two underlying facts - a) there exists a large enough length-scales for which the acquisition function can be made to have a significant gradient at any location in the parameter space, and b) the extrema of the consecutive acquisition functions are close although they are different only due to a small difference in the length-scales. Theoretical guarantees are provided and experiments clearly demonstrate the utility of the proposed method at high dimension using both benchmark test functions and real-world case studies.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	  	          <a href="http://proceedings.mlr.press/v70/rana17a/rana17a.pdf" class="label label-info">PDF</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="https://www.ijcai.org/proceedings/2017/0355.pdf">
    <img src="./img/paper/ijcai2017.png" alt="">
  </a>
  <div class="media-body">
     <strong>V. Nguyen</strong>, D. Phung, T. Le, H. Bui <br>
	    <strong>Discriminative Bayesian Nonparametric Clustering</strong><br>

         <i>Proceedings of International Joint Conference on Artificial Intelligence</i>, (<strong>IJCAI</strong>), pp 2550-2556,    2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractIJCAI_Vu" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractIJCAI_Vu" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Discriminative Bayesian Nonparametric Clustering</h4>
            </div>
            <div class="modal-body">
			We propose a general framework for discriminative
Bayesian nonparametric clustering to promote
the inter-discrimination among the learned clusters
in a fully Bayesian nonparametric (BNP) manner.
Our method combines existing BNP clustering and
discriminative models by enforcing latent cluster
indices to be consistent with the predicted labels
resulted from probabilistic discriminative model.
This formulation results in a well-defined generative
process wherein we can use either logistic regression
or SVM for discrimination. Using the proposed
framework, we develop two novel discriminative
BNP variants: the discriminative Dirichlet
process mixtures, and the discriminative-state in-
finite HMMs for sequential data. We develop ef-
ficient data-augmentation Gibbs samplers for posterior
inference. Extensive experiments in image
clustering and dynamic location clustering demonstrate
that by encouraging discrimination between
induced clusters, our model enhances the quality of
clustering in comparison with the traditional generative
BNP models.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	  	          <a href="https://www.ijcai.org/proceedings/2017/0355.pdf" class="label label-info">PDF</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="https://www.ijcai.org/proceedings/2017/0291.pdf">
    <img src="./img/paper/rsz_global_optimization.png" alt="">
  </a>
  <div class="media-body">
    C. Li, S. Gupta, S. Rana, <strong>V. Nguyen</strong>, S. Venkatesh, A. Shilton <br>
	    <strong>High Dimensional Bayesian Optimization Using Dropout</strong><br>

         <i>Proceedings of International Joint Conference on Artificial Intelligence  </i>, (<strong>IJCAI</strong>), pp 2096-2102,   2017.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractIJCAI_Cheng" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractIJCAI_Cheng" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">High Dimensional Bayesian Optimization Using Dropout</h4>
            </div>
            <div class="modal-body">
			Scaling Bayesian optimization to high dimensions
is challenging task as the global optimization of
high-dimensional acquisition function can be expensive
and often infeasible. Existing methods depend
either on limited “active” variables or the additive
form of the objective function. We propose
a new method for high-dimensional Bayesian optimization,
that uses a dropout strategy to optimize
only a subset of variables at each iteration. We derive
theoretical bounds for the regret and show how
it can inform the derivation of our algorithm. We
demonstrate the efficacy of our algorithms for optimization
on two benchmark functions and two realworld
applications - training cascade classifiers and
optimizing alloy composition
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	  	          <a href="https://www.ijcai.org/proceedings/2017/0291.pdf" class="label label-info">PDF</a>

  </div>
</div>



  <h3>2016</h3>




<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v63/nguyen93.html">
    <img src="./img/paper/acml2016.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. K. Gupta, S. Rana, C. Li, S. Venkatesh <br>
	    <strong>A Bayesian Nonparametric Approach for Multi-label Classification</strong><br>

         <i>Proceedings of The 8th Asian Conference on Machine Learning</i>, (<strong>ACML</strong>), pp 254-269,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractACML_BNMC" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractACML_BNMC" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">A Bayesian Nonparametric Approach for Multi-label Classification</h4>
            </div>
            <div class="modal-body">
			Many real-world applications require multi-label classification where multiple target labels
are assigned to each instance. In multi-label classification, there exist the intrinsic correlations
between the labels and features. These correlations are beneficial for multi-label
classification task since they reflect the coexistence of the input and output spaces that can
be exploited for prediction. Traditional classification methods have attempted to reveal
these correlations in different ways. However, existing methods demand expensive computation
complexity for finding such correlation structures. Furthermore, these approaches
can not identify the suitable number of label-feature correlation patterns. In this paper, we
propose a Bayesian nonparametric (BNP) framework for multi-label classification that can
automatically learn and exploit the unknown number of multi-label correlation. We utilize
the recent techniques in stochastic inference to derive the cheap (but efficient) posterior
inference algorithm for the model. In addition, our model can naturally exploit the useful
information from missing label samples. Furthermore, we extend the model to update parameters
in an online fashion that highlights the flexibility of our model against the existing
approaches. We compare our method with the state-of-the-art multi-label classification algorithms
on real-world datasets using both complete and missing label settings. Our model
achieves better classification accuracy while our running time is consistently much faster
than the baselines in an order of magnitude.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	  	          <a href="http://proceedings.mlr.press/v63/nguyen93.html" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ACML2016_BNMC" class="label label-success">Code</a>
		<a href="https://www.youtube.com/watch?v=-EE-I2IpQbo" class="label label-success">Youtube Demo</a>
		<a href="" class="label label-danger">Best Paper Runner Up Award</a>
		<a href="" class="label label-danger">Best Poster Award</a>

	  
  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="http://proceedings.mlr.press/v63/nguyen19.html">
    <img src="./img/paper/acml16_kernel.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
    K. Nguyen, T. Le, <strong>V. Nguyen</strong>, T. D. Nguyen, D. Phung <br>
	    <strong>Multiple Kernel Learning with Data Augmentation</strong><br>

         <i>Proceedings of The 8th Asian Conference on Machine Learning</i>, (<strong>ACML</strong>),  pp 49-64,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractACML_MKL" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractACML_MKL" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Multiple Kernel Learning with Data Augmentation</h4>
            </div>
            <div class="modal-body">
			The motivations of multiple kernel learning (MKL) approach are to increase kernel expressiveness
capacity and to avoid the expensive grid search over a wide spectrum of kernels.
A large amount of work has been proposed to improve the MKL in terms of the computational
cost and the sparsity of the solution. However, these studies still either require an
expensive grid search on the model parameters or scale unsatisfactorily with the numbers
of kernels and training samples. In this paper, we address these issues by conjoining MKL,
Stochastic Gradient Descent (SGD) framework, and data augmentation technique. The
pathway of our proposed method is developed as follows. We first develop a maximum-aposteriori
(MAP) view for MKL under a probabilistic setting and described in a graphical
model. This view allows us to develop data augmentation technique to make the inference
for finding the optimal parameters feasible, as opposed to traditional approach of training
MKL via convex optimization techniques. As a result, we can use the standard SGD
framework to learn weight matrix and extend the model to support online learning. We
validate our method on several benchmark datasets in both batch and online settings. The
experimental results show that our proposed method can learn the parameters in a principled
way to eliminate the expensive grid search while gaining a significant computational
speedup comparing with the state-of-the-art baselines.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	  	          <a href="http://proceedings.mlr.press/v63/nguyen19.html" class="label label-info">PDF</a>



	  
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/7837957/">
    <img src="./img/paper/b3o.PNG" alt="" width="140" height="100">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, S. Rana, S. K. Gupta, C. Li, S. Venkatesh <br>
	    <strong>Budgeted Batch Bayesian Optimization</strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 1107-1112,    2016. [180/918=19%]<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractB3O_ICDM" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractB3O_ICDM" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Budgeted Batch Bayesian Optimization</h4>
            </div>
            <div class="modal-body">
			Parameter settings profoundly impact the performance
of machine learning algorithms and laboratory experiments.
The classical trial-error methods are exponentially expensive
in large parameter spaces, and Bayesian optimization (BO)
offers an elegant alternative for global optimization of black box
functions. In situations where the functions can be evaluated
at multiple points simultaneously, batch Bayesian optimization
is used. Current batch BO approaches are restrictive in fixing
the number of evaluations per batch, and this can be wasteful
when the number of specified evaluations is larger than the
number of real maxima in the underlying acquisition function.
We present the budgeted batch Bayesian optimization (B3O) for
hyper-parameter tuning and experimental design - we identify
the appropriate batch size for each iteration in an elegant
way. In particular, we use the infinite Gaussian mixture model
(IGMM) for automatically identifying the number of peaks in
the underlying acquisition functions. We solve the intractability
of estimating the IGMM directly from the acquisition function
by formulating the batch generalized slice sampling to efficiently
draw samples from the acquisition function. We perform extensive
experiments for benchmark functions and two real world
applications - machine learning hyper-parameter tuning and
experimental design for alloy hardening. We show empirically
that the proposed B3O outperforms the existing fixed batch BO
approaches in finding the optimum whilst requiring a fewer
number of evaluations, thus saving cost and time.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	  	          <a href="http://ieeexplore.ieee.org/document/7837957/" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ICDM2016_B3O" class="label label-success">Code</a>
	  
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/document/7837958/">
    <img src="./img/paper/ICDM2016_OLR.png" alt="">
  </a>
  <div class="media-body">
    <strong>V. Nguyen</strong>, T. D. Nguyen, T. Le, S. Venkatesh, D. Phung  <br>
	    <strong>One-pass Logistic Regression for Label-drift and Large-scale Classification on Distributed Systems </strong><br>

         <i>Proceedings of the IEEE International Conference on Data Mining</i>, (<strong>ICDM</strong>),  pp 1113-1118,    2016. [180/918=19%]<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractOLR_ICDM" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractOLR_ICDM" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">One-pass Logistic Regression for Label-drift and Large-scale Classification on Distributed Systems</h4>
            </div>
            <div class="modal-body">
			Logistic regression (LR) is at the cornerstone of classification. Its extension for multiclass classification is the
workhorse in industry, where a set of predefined classes is required. The model, however, fails to work in the case where
the class labels are not known in advance, a problem we term label-drift classification, in a similar spirit of the so-called conceptdrift
problem in the literature. Label-drift classification problem naturally occurs in many applications, especially in the context
of streaming and online settings where the incoming data may contain samples categorized with new classes that have not
been previously seen. Additionally, in the wave of big data, traditional LR methods may fail due to their expense of running
time and label-drift requirements. In this paper, we introduce a novel variant of LR, namely one-pass logistic regression (OLR)
to offer a principled treatment for large-scale and label-drift classifications. Our key contribution is the derivation of sufficient
statistic update for MAP estimation of Polya-Gamma augmentation for LR. Manipulating these sufficient statistics is convenient,
allowing our proposed method to efficiently perform the labeldrift classification under an online setting without retraining the
model from scratch. To handle large-scale classification for big data, we further extend our OLR to a distributed setting for
parallelization, termed sparkling OLR (Spark-OLR). We demonstrate the scalability of our proposed methods on large-scale
datasets with more than one hundred million data points. The experimental results show that the predictive performances of our
methods are comparable or better than those of state-of-the-art baselines whilst the execution time is much faster at an order of
magnitude. To measure the inherent trade-off between speed and accuracy, we propose quadrant visualization and quadrant score,
on which our proposed model outperforms other methods on all datasets. In addition, the OLR and Spark-OLR are invariant
to data shuffling and have no hyperparameter to tune that significantly benefits data practitioners and overcomes the curse
of big data cross-validation to select optimal hyperparameters.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  	          <a href="http://ieeexplore.ieee.org/document/7837958/" class="label label-info">PDF</a>

	  <a href="https://github.com/ntienvu/ICDM2016_OLR" class="label label-success">Code</a>
  </div>
</div>




<div class="media">
  <a class="pull-left thumbnail" href="https://papers.nips.cc/paper/6560-dual-space-gradient-descent-for-online-learning.pdf">
    <img src="./img/paper/NIPS2016.png" alt="">
  </a>
  <div class="media-body">
    T. Le, T.D. Nguyen,<strong>V. Nguyen</strong>, D. Phung  <br>
	    <strong>Dual Space Gradient Descent for Online Learning</strong><br>

         <i>Advances in Neural Information Processing Systems</i>, (<strong>NIPS</strong>),  pp 4583-4591,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractNIPS" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractNIPS" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Dual Space Gradient Descent for Online Learning</h4>
            </div>
            <div class="modal-body">
			One crucial goal in kernel online learning is to bound the model size. Common
approaches employ budget maintenance procedures to restrict the model sizes using
removal, projection, or merging strategies. Although projection and merging, in the
literature, are known to be the most effective strategies, they demand extensive com
putation whilst removal strategy fails to retain information of the removed vectors.
An alternative way to address the model size problem is to apply random features
to approximate the kernel function. This allows the model to be maintained directly
in the random feature space, hence effectively resolve the curse of kernelization.
However, this approach still suffers from a serious shortcoming as it needs to use a
high dimensional random feature space to achieve a sufficiently accurate kernel
 approximation. Consequently, it leads to a significant increase in the computational
 cost. To address all of these aforementioned challenges, we present in this paper
 the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes
 random features as an auxiliary space to maintain information from data points
 removed during budget maintenance. Consequently, our approach permits the
budget to be maintained in a simple, direct and elegant way while simultaneously
 mitigating the impact of the dimensionality issue on learning performance. We
 further provide convergence analysis and extensively conduct experiments on five
 real-world datasets to demonstrate the predictive performance and scalability of
 our proposed method in comparison with the state-of-the-art baselines.
</div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
	          <a href="https://papers.nips.cc/paper/6560-dual-space-gradient-descent-for-online-learning.pdf" class="label label-info">PDF</a>
		  <a href="https://github.com/tund/dsgd" class="label label-success">Code</a>

			  
	    </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://www.sciencedirect.com/science/article/pii/S1574119216302097">
    <img src="./img/paper/PERCOM2016.jpg" alt="">
  </a>
  <div class="media-body">
    T. Nguyen, <strong>V. Nguyen</strong>, F.D. Salim, D.V. Le, D. Phung  <br>
	    <strong>A Simultaneous Extraction of Context and Community from pervasive signals using nested Dirichlet process</strong><br>

        <i>Pervasive and Mobile Computing</i>, Elsevier,  2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractPMC16" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractPMC16" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">A Simultaneous Extraction of Context and Community from pervasive signals using nested Dirichlet process</h4>
            </div>
            <div class="modal-body">
Understanding user contexts and group structures plays a central role in pervasive computing. These contexts and community structures are complex to mine from data collected in the wild due to the unprecedented growth of data, noise, uncertainties and complexities. Typical existing approaches would first extract the latent patterns to explain the human dynamics or behaviors and then use them as a way to consistently formulate numerical representations for community detection, often via a clustering method. While being able to capture high-order and complex representations, these two steps are performed separately. More importantly, they face a fundamental difficulty in determining the correct number of latent patterns and communities. This paper presents an approach that seamlessly addresses these challenges to simultaneously discover latent patterns and communities in a unified Bayesian nonparametric framework. Our Simultaneous Extraction of Context and Community (SECC) model roots in the nested Dirichlet process theory which allows nested structure to be built to summarize data at multiple levels. We demonstrate our framework on five datasets where the advantages of the proposed approach are validated.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://www.sciencedirect.com/science/article/pii/S1574119216302097" class="label label-info">PDF</a>
  </div>
</div>







<div class="media">
  <a class="pull-left thumbnail" href="http://jmlr.org/proceedings/papers/v51/le16.pdf">
    <img src="./img/paper/AISTATS2016.jpg" alt="">
  </a>
  <div class="media-body">
    T. Le, <strong>V. Nguyen</strong>, TD. Nguyen, D. Phung  <br>
	    <strong>Nonparametric Budgeted Stochastic Gradient Descent</strong><br>
         <i>Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</i>, (<strong>AISTATS</strong>),  pp 654-572,    2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractabstractNonparametricBudgetedStochasticGradientDescent" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractabstractNonparametricBudgetedStochasticGradientDescent" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Nonparametric Budgeted Stochastic Gradient Descent</h4>
            </div>
            <div class="modal-body">
One of the most challenging problems in kernel online learning is to bound the model size. 
Budgeted kernel online learning addresses this issue by bounding the model size to a predefined budget. 
However, determining an appropriate value for such predefined budget is arduous. 
In this paper, we propose the Nonparametric Budgeted Stochastic Gradient Descent that allows the model size to automatically grow with data in a principled way. 
We provide theoretical analysis to show that our framework is guaranteed to converge for a large collection of loss functions 
(e.g. Hinge, Logistic, L2, L1, and ε-insensitive) which enables the proposed algorithm to perform both classification and regression tasks without hurting the ideal convergence rate O(1/T) 
of the standard Stochastic Gradient Descent. We validate our algorithm on the real-world datasets to consolidate the theoretical claims.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://jmlr.org/proceedings/papers/v51/le16.pdf" class="label label-info">PDF</a>
      <a href="https://github.com/ntienvu/NonparametricBudgetedSGD" class="label label-success">Code</a>
  </div>
</div>

<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7456501&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7456501">
    <img src="./img/paper/PERCOM2016.jpg" alt="">
  </a>
  <div class="media-body">
    T. Nguyen, <strong>V. Nguyen</strong>, FD. Salim, D. Phung  <br>
	    <strong>SECC: Simultaneous Extraction of Context and Community from Pervasive Signals</strong><br>
         <i>Proceedings of 2016 IEEE International Conference on Pervasive Computing and Communications</i>, (<strong>PERCOM</strong>), pp 1-9,  2016.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractSECC" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractSECC" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">SECC: Simultaneous Extraction of Context and Community from Pervasive Signals</h4>
            </div>
            <div class="modal-body">
Understanding user contexts and group structures plays a central role in pervasive computing. These contexts and community structures are complex to mine from data collected in the 
wild due to the unprecedented growth of data, noise, uncertainties and complexities. Typical existing approaches would first extract the latent patterns to explain the human dynamics 
or behaviors and then use them as the way to consistently formulate numerical representations for community detection, often via a clustering method. While being able to capture highorder 
and complex representations, these two steps are performed separately. More importantly, they face a fundamental difficulty in determining the correct number of latent patterns and communities. 
This paper presents an approach that seamlessly addresses these challenges to simultaneously discover latent patterns and communities in a unified Bayesian nonparametric framework. 
Our Simultaneous Extraction of Context and Community (SECC) model roots in the nested Dirichlet process theory which allows nested structure to be built to explain data at multiple levels. 
We demonstrate our framework on three public datasets where the advantages of the proposed approach are validated.            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7456501&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7456501" class="label label-info">PDF</a>
  </div>
</div>

  <h3>Pre-2015</h3>




<div class="media">
  <a class="pull-left thumbnail" href="http://dro.deakin.edu.au/eserv/DU:30079715/nguyen-bayesianmultilevel-2015A.pdf">
    <img src="./img/paper/rsz_thesis-clipart-article.png" alt="">
  </a>
  <div class="media-body">
   <strong>V. Nguyen</strong><br>
   <strong>Bayesian Nonparametric Multilevel Modelling and Applications</strong><br>
        <i>Deakin University</i>, Thesis, December 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractThesis" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractThesis" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Nonparametric Multilevel Modelling and Applications</h4>
            </div>
            <div class="modal-body">

            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://dro.deakin.edu.au/eserv/DU:30079715/nguyen-bayesianmultilevel-2015A.pdf" class="label label-info">PDF</a>
  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://approximateinference.org/accepted/NguyenEtAl2015.pdf">
    <img src="./img/paper/NIPS2015_AABI.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>V. Nguyen</strong>, D. Phung, T. Le, S. Venkatesh   <br>
   <strong>Large Sample Asymptotic for Nonparametric Mixture Model with Count Data.</strong><br>
         <i>Workshop on Advances in Approximate Bayesian Inference at Neural Information Processing Systems</i>, (<strong>NIPSW</strong>), 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractLSA" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractLSA" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Large Sample Asymptotic for Nonparametric Mixture Model with Count Data</h4>
            </div>
            <div class="modal-body">
Bayesian nonparametric models have become popular recently due to its flexibility
in identifying the unknown number of clusters. However, the flexibility comes at
a cost for learning. Thus, Small Variance Asymptotic (SVA) is one of the promising
approach for scalability in Bayesian nonparametric models. SVA approach for
count data is also developed in which the likelihood function is replaced by the
Kullback–Leibler divergence. In this paper, we present the Large Sample Asymptotic
for count data when the number of sample in Multinomial distribution goes
to infinity, we derive the similar result to SVA for scalable clustering.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://approximateinference.org/accepted/NguyenEtAl2015.pdf" class="label label-info">PDF</a>
	  <a href="https://github.com/ntienvu/LargeSampleAsymptotic_ScalableNonparametricClustering" class="label label-success">Code</a>
	  <a href="http://prada-research.net/~tienvu/slide/nips_aabi15_workshop.pdf" class="label label-warning">Poster</a>

  </div>
</div>


<div class="media">
  <a class="pull-left thumbnail" href="http://link.springer.com/article/10.1007/s40745-015-0030-3">
    <img src="./img/paper/ADS2015.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>V. Nguyen</strong>, D. Phung, D.S. Pham, and S. Venkatesh   <br>
       <strong>Bayesian Nonparametric Approaches to Abnormality Detection in Video Surveillance.</strong><br>
         <i>Annals of Data Science</i>, pp 1-21, 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractBayesAbnormal" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractBayesAbnormal" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Nonparametric Approaches to Abnormality Detection in Video Surveillance</h4>
            </div>
            <div class="modal-body">
In data science, anomaly detection is the process of identifying the items, events or observations which do not conform to expected patterns in a dataset. As widely acknowledged 
in the computer vision community and security management, discovering suspicious events is the key issue for abnormal detection in video surveillance. The important steps in identifying 
such events include stream data segmentation and hidden patterns discovery. However, the crucial challenge in stream data segmentation and hidden patterns discovery are the number of coherent 
segments in surveillance stream and the number of traffic patterns are unknown and hard to specify. Therefore, in this paper we revisit the abnormality detection problem through the lens of 
Bayesian nonparametric (BNP) and develop a novel usage of BNP methods for this problem. In particular, we employ the Infinite Hidden Markov Model and Bayesian Nonparametric Factor Analysis for 
stream data segmentation and pattern discovery. 
In addition, we introduce an interactive system allowing users to inspect and browse suspicious events.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://link.springer.com/article/10.1007/s40745-015-0030-3" class="label label-info">PDF</a>
		  <a href="https://github.com/ntienvu/abnormal_detection_video_surveillance" class="label label-success">Code</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://link.springer.com/chapter/10.1007/978-3-319-18038-0_26">
    <img src="./img/paper/PAKDD2015.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>V. Nguyen</strong>, D. Phung, S. Venkatesh, and H. Bui   <br>
       <strong>A Bayesian Nonparametric Approach to Multilevel Regression.</strong><br>
         <i>Advances in Knowledge Discovery and Data Mining</i> (PAKDD), pp 330-342, 2015.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractMulReg" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractMulReg" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">A Bayesian Nonparametric Approach to Multilevel Regression</h4>
            </div>
            <div class="modal-body">
Regression is at the cornerstone of statistical analysis. Multilevel regression, on the other hand, receives little research attention, though it is prevalent in economics, biostatistics 
and healthcare to name a few. We present a Bayesian nonparametric framework for multilevel regression where individuals including observations and outcomes are organized into groups. 
Furthermore, our approach exploits additional group-specific context observations, we use Dirichlet Process with product-space base measure in a nested structure to model group-level context 
distribution and the regression distribution to accommodate the multilevel structure of the data. The proposed model simultaneously partitions groups into cluster and perform regression. 
We provide collapsed Gibbs sampler for posterior inference.
 We perform extensive experiments on econometric panel data and healthcare longitudinal data to demonstrate the effectiveness of the proposed model.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://link.springer.com/chapter/10.1007/978-3-319-18038-0_26" class="label label-info">PDF</a>
		 <a href="http://prada-research.net/~tienvu/slide/pakdd_2015.pdf" class="label label-warning">Slide</a>

  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://jmlr.org/proceedings/papers/v32/nguyenb14.html">
    <img src="./img/paper/ICML2014.jpg" alt="">
  </a>
  <div class="media-body">
   <strong>V. Nguyen</strong>, D. Phung, L. Nguyen, S. Venkatesh and H. Bui   <br>
       <strong>Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts.</strong><br>

         <i>Proceedings of The 31st International Conference on Machine Learning</i> (<strong>ICML</strong>), pp. 288–296, 2014.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractICML14" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractICML14" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts</h4>
            </div>
            <div class="modal-body">
We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents 
and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context 
observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: 
integrating out all contents results in the DPM over contexts, whereas integrating out group-speciﬁc contexts results in the nDP mixture over content variables. We provide a Polya-urn view of 
the model and an efﬁcient collapsed Gibbs inference procedure. 
Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://jmlr.org/proceedings/papers/v32/nguyenb14.html" class="label label-info">PDF</a>
		 <a href="http://prada-research.net/~tienvu/slide/ICML_2014.pdf" class="label label-warning">Slide</a>


  </div>
</div>



<div class="media">
  <a class="pull-left thumbnail" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6529821&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6529821">
    <img src="./img/paper/ISSNIP2013.jpg" alt="">
  </a>
  <div class="media-body">
     <strong>T.V. Nguyen</strong>, D. Phung, S. K. Gupta, and S. Venkatesh  <br>
    <strong>Interactive Browsing System for Anomaly Video Surveillance.</strong><br>
         <i>IEEE Eighth International Conference on Intelligent Sensors, Sensor Networks and Information Processing</i> (ISSNIP), pp 384-389, 2013.<br>
				  
      <a data-toggle="modal" href="http://prada-research.net/~tienvu/#abstractISSNIP13" class="label label-default">Abstract</a>
      <!-- Modal -->
      <div class="modal fade" id="abstractISSNIP13" tabindex="-1" role="dialog" aria-hidden="true"> <div class="modal-dialog">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
              <h4 class="modal-title">Interactive Browsing System for Anomaly Video Surveillance</h4>
            </div>
            <div class="modal-body">
Existing anomaly detection methods in video surveillance exhibit lack of congruence between rare events detected by algorithms and what is considered anomalous by users. 
This paper introduces a novel browsing model to address this issue, allowing users to interactively examine rare events in an intuitive manner. Introducing a novel way to compute rare 
motion patterns, we estimate latent factors of foreground motion patterns through Bayesian Nonparametric Factor analysis. Each factor corresponds to a typical motion pattern. 
A rarity score for each factor is computed, and ordered in decreasing order of rarity, permitting users to browse events using any proportion of rare factors. Rare events correspond to frames
 that contain the rare factors chosen. We present the user with an interface to inspect events that incorporate these rarest factors in a spatial-temporal manner.
 We demonstrate the system on a public video data set, showing key aspects of the browsing paradigm.
            </div>
          </div><!-- /.modal-content -->
        </div><!-- /.modal-dialog -->
      </div><!-- /.modal --> 
	  
        <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6529821&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6529821" class="label label-info">PDF</a>
		  <a href="https://github.com/ntienvu/abnormal_detection_video_surveillance" class="label label-success">Code</a>
		<a href="http://prada-research.net/~tienvu/slide/issnip13_poster.pdf" class="label label-warning">Poster</a>


  </div>
</div>






		</div>
      </div>
		</div>
      </div>
      </div>
      </div>
      <footer class="text-center text-muted">
        <hr>
        Last updated Sep 27, 2020.<br>
	Based on the code of 
        <a href="https://github.com/alopez/alopez.github.com">Adam Lopez</a>.<br>
        Created with 
        <a href="http://git-scm.com/">git</a>,
        <a href="http://jekyllrb.com/">jekyll</a>,
        <a href="http://getbootstrap.com/">bootstrap</a>,
        and <a href="http://www.vim.org/">vim</a>.<br> 
        <br><br>
      </footer>
    </div>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
         m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-46770577-1', 'jhu.edu');
      ga('send', 'pageview');
    </script>
  


</body></html>

